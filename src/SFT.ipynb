{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/home/chawak/models/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b64a7e8ff6149d69b552ff36dcdfeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6a1d767e264317a0c5ffe57c0dcea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46d21a72c4840b0852a27fa8f62752e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4eebf97a5e45f88ab0226945065d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde1b6b66c2c4b7193a6f91d6dc5bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08911377beb94497a3780b17be91f25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7937778feafd48cfb044536374e49dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411460ad9d924ec5b63e6f8e2cfcde14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98d545cc5254f95bc2554dd77661a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for tokenizing\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        text_target=examples['gold_plan'],\n",
    "        truncation=False,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "split='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "data=load_dataset(\"csv\",data_files=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'gold_plan'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb82de144da44d5190c3bbaa2ae78226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Gemma2ForCausalLM' object has no attribute 'model_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_data\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39;49mmap(tokenize_function,batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/datasets/dataset_dict.py:941\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m with_split:\n\u001b[1;32m    939\u001b[0m     function \u001b[39m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 941\u001b[0m dataset_dict[split] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    942\u001b[0m     function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    943\u001b[0m     with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    944\u001b[0m     with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    945\u001b[0m     input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    946\u001b[0m     batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    947\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    948\u001b[0m     drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    949\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    950\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    951\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    952\u001b[0m     cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[split],\n\u001b[1;32m    953\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    954\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    955\u001b[0m     disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    956\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    957\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    958\u001b[0m     desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m with_split:\n\u001b[1;32m    962\u001b[0m     function \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunc\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: \u001b[39mlist\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/datasets/arrow_dataset.py:3074\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3069\u001b[0m     \u001b[39mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3070\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3071\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3072\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3073\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3074\u001b[0m         \u001b[39mfor\u001b[39;49;00m rank, done, content \u001b[39min\u001b[39;49;00m Dataset\u001b[39m.\u001b[39;49m_map_single(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdataset_kwargs):\n\u001b[1;32m   3075\u001b[0m             \u001b[39mif\u001b[39;49;00m done:\n\u001b[1;32m   3076\u001b[0m                 shards_done \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/datasets/arrow_dataset.py:3516\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3514\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3515\u001b[0m     _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m-> 3516\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, batch \u001b[39min\u001b[39;49;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3517\u001b[0m         num_examples_in_batch \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(i)\n\u001b[1;32m   3518\u001b[0m         \u001b[39mif\u001b[39;49;00m update_data:\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/datasets/arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3465\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3466\u001b[0m         \u001b[39myield\u001b[39;00m i, apply_function(example, i, offset\u001b[39m=\u001b[39;49moffset)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/datasets/arrow_dataset.py:3389\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3387\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[39m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[39m=\u001b[39moffset)\n\u001b[0;32m-> 3389\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3390\u001b[0m \u001b[39mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[88], line 7\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtokenize_function\u001b[39m(examples):\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(\n\u001b[1;32m      4\u001b[0m         examples[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m         text_target\u001b[39m=\u001b[39mexamples[\u001b[39m'\u001b[39m\u001b[39mgold_plan\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m----> 7\u001b[0m         padding\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mmodel_max_length,\n\u001b[1;32m      8\u001b[0m     )\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Gemma2ForCausalLM' object has no attribute 'model_max_length'"
     ]
    }
   ],
   "source": [
    "tokenized_data=data.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'I am playing with a set of blocks where I need to arrange the blocks into stacks\\n        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\\n        I have the following restrictions on my actions:\\n        I can only pick up or unstack one block at a time\\n        I can only pick up or unstack a block if my hand is empty\\n        I can only pick up a block if the block is on the table and the block is clear\\n        A block is clear if the block has no other blocks on top of it and if the block is not picked up\\n        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\\n        I can only unstack a block from on top of another block if the block I am unstacking is clear\\n        Once I pick up or unstack a block, I am holding the block\\n        I can only put down a block that I am holding\\n        I can only stack a block on top of another block if I am holding the block being stacked\\n        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\\n        Once I put down or stack a block, my hand becomes empty\\n        Once you stack a block on top of a second block, the second block is no longer clear\\n\\n[STATEMENT]\\nAs initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\\nMy goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nunstack the brown block from on top of the violet block\\nput down the brown block\\npick up the violet block\\nstack the violet block on top of the brown block\\npick up the teal block\\nstack the teal block on top of the violet block\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\\nMy goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\\n\\nMy plan is as follows: \\n\\n[PLAN]\\n\\n',\n",
       " 'gold_plan': '[PLAN]\\nunstack the green block from on top of the red block\\nput down the green block\\n[PLAN END]',\n",
       " 'input_ids': [2,\n",
       "  235285,\n",
       "  1144,\n",
       "  6155,\n",
       "  675,\n",
       "  476,\n",
       "  1142,\n",
       "  576,\n",
       "  13854,\n",
       "  1570,\n",
       "  590,\n",
       "  1476,\n",
       "  577,\n",
       "  9900,\n",
       "  573,\n",
       "  13854,\n",
       "  1280,\n",
       "  63297,\n",
       "  108,\n",
       "  145,\n",
       "  4858,\n",
       "  708,\n",
       "  573,\n",
       "  8737,\n",
       "  590,\n",
       "  798,\n",
       "  749,\n",
       "  235292,\n",
       "  17350,\n",
       "  908,\n",
       "  476,\n",
       "  3963,\n",
       "  235269,\n",
       "  2132,\n",
       "  8388,\n",
       "  476,\n",
       "  3963,\n",
       "  774,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  2550,\n",
       "  3963,\n",
       "  235269,\n",
       "  13298,\n",
       "  1706,\n",
       "  476,\n",
       "  3963,\n",
       "  235269,\n",
       "  23850,\n",
       "  476,\n",
       "  3963,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  2550,\n",
       "  3963,\n",
       "  235265,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  791,\n",
       "  573,\n",
       "  2412,\n",
       "  16842,\n",
       "  611,\n",
       "  970,\n",
       "  8737,\n",
       "  235292,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  4788,\n",
       "  908,\n",
       "  689,\n",
       "  748,\n",
       "  8388,\n",
       "  974,\n",
       "  3963,\n",
       "  696,\n",
       "  476,\n",
       "  1069,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  4788,\n",
       "  908,\n",
       "  689,\n",
       "  748,\n",
       "  8388,\n",
       "  476,\n",
       "  3963,\n",
       "  1013,\n",
       "  970,\n",
       "  1634,\n",
       "  603,\n",
       "  8144,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  4788,\n",
       "  908,\n",
       "  476,\n",
       "  3963,\n",
       "  1013,\n",
       "  573,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  578,\n",
       "  573,\n",
       "  3963,\n",
       "  603,\n",
       "  3110,\n",
       "  108,\n",
       "  145,\n",
       "  235280,\n",
       "  3963,\n",
       "  603,\n",
       "  3110,\n",
       "  1013,\n",
       "  573,\n",
       "  3963,\n",
       "  919,\n",
       "  793,\n",
       "  1156,\n",
       "  13854,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  665,\n",
       "  578,\n",
       "  1013,\n",
       "  573,\n",
       "  3963,\n",
       "  603,\n",
       "  780,\n",
       "  15532,\n",
       "  908,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  748,\n",
       "  8388,\n",
       "  476,\n",
       "  3963,\n",
       "  774,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  2550,\n",
       "  3963,\n",
       "  1013,\n",
       "  573,\n",
       "  3963,\n",
       "  590,\n",
       "  1144,\n",
       "  748,\n",
       "  8388,\n",
       "  574,\n",
       "  729,\n",
       "  2277,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  1156,\n",
       "  3963,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  748,\n",
       "  8388,\n",
       "  476,\n",
       "  3963,\n",
       "  774,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  2550,\n",
       "  3963,\n",
       "  1013,\n",
       "  573,\n",
       "  3963,\n",
       "  590,\n",
       "  1144,\n",
       "  748,\n",
       "  8388,\n",
       "  574,\n",
       "  603,\n",
       "  3110,\n",
       "  108,\n",
       "  145,\n",
       "  14326,\n",
       "  590,\n",
       "  4788,\n",
       "  908,\n",
       "  689,\n",
       "  748,\n",
       "  8388,\n",
       "  476,\n",
       "  3963,\n",
       "  235269,\n",
       "  590,\n",
       "  1144,\n",
       "  8576,\n",
       "  573,\n",
       "  3963,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  2507,\n",
       "  1706,\n",
       "  476,\n",
       "  3963,\n",
       "  674,\n",
       "  590,\n",
       "  1144,\n",
       "  8576,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  13410,\n",
       "  476,\n",
       "  3963,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  2550,\n",
       "  3963,\n",
       "  1013,\n",
       "  590,\n",
       "  1144,\n",
       "  8576,\n",
       "  573,\n",
       "  3963,\n",
       "  1855,\n",
       "  56368,\n",
       "  108,\n",
       "  145,\n",
       "  235285,\n",
       "  798,\n",
       "  1297,\n",
       "  13410,\n",
       "  476,\n",
       "  3963,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  2550,\n",
       "  3963,\n",
       "  1013,\n",
       "  573,\n",
       "  3963,\n",
       "  10401,\n",
       "  948,\n",
       "  590,\n",
       "  1144,\n",
       "  87337,\n",
       "  573,\n",
       "  3963,\n",
       "  603,\n",
       "  3110,\n",
       "  108,\n",
       "  145,\n",
       "  14326,\n",
       "  590,\n",
       "  2507,\n",
       "  1706,\n",
       "  689,\n",
       "  13410,\n",
       "  476,\n",
       "  3963,\n",
       "  235269,\n",
       "  970,\n",
       "  1634,\n",
       "  8485,\n",
       "  8144,\n",
       "  108,\n",
       "  145,\n",
       "  14326,\n",
       "  692,\n",
       "  13410,\n",
       "  476,\n",
       "  3963,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  476,\n",
       "  2257,\n",
       "  3963,\n",
       "  235269,\n",
       "  573,\n",
       "  2257,\n",
       "  3963,\n",
       "  603,\n",
       "  793,\n",
       "  5543,\n",
       "  3110,\n",
       "  109,\n",
       "  235309,\n",
       "  143005,\n",
       "  235307,\n",
       "  108,\n",
       "  2169,\n",
       "  5528,\n",
       "  4202,\n",
       "  590,\n",
       "  791,\n",
       "  674,\n",
       "  235269,\n",
       "  573,\n",
       "  8426,\n",
       "  3963,\n",
       "  603,\n",
       "  3110,\n",
       "  235269,\n",
       "  573,\n",
       "  88735,\n",
       "  3963,\n",
       "  603,\n",
       "  3110,\n",
       "  235269,\n",
       "  139,\n",
       "  1175,\n",
       "  1634,\n",
       "  603,\n",
       "  8144,\n",
       "  235269,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  235269,\n",
       "  573,\n",
       "  8426,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  235269,\n",
       "  573,\n",
       "  88735,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  235265,\n",
       "  108,\n",
       "  2926,\n",
       "  6789,\n",
       "  603,\n",
       "  577,\n",
       "  791,\n",
       "  674,\n",
       "  139,\n",
       "  1175,\n",
       "  8426,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  235269,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  8426,\n",
       "  3963,\n",
       "  235269,\n",
       "  573,\n",
       "  88735,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  235265,\n",
       "  109,\n",
       "  2926,\n",
       "  1780,\n",
       "  603,\n",
       "  685,\n",
       "  6397,\n",
       "  235292,\n",
       "  109,\n",
       "  235309,\n",
       "  42446,\n",
       "  235307,\n",
       "  108,\n",
       "  549,\n",
       "  8388,\n",
       "  573,\n",
       "  8426,\n",
       "  3963,\n",
       "  774,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  108,\n",
       "  1065,\n",
       "  1706,\n",
       "  573,\n",
       "  8426,\n",
       "  3963,\n",
       "  108,\n",
       "  18075,\n",
       "  908,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  108,\n",
       "  8388,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  8426,\n",
       "  3963,\n",
       "  108,\n",
       "  18075,\n",
       "  908,\n",
       "  573,\n",
       "  88735,\n",
       "  3963,\n",
       "  108,\n",
       "  8388,\n",
       "  573,\n",
       "  88735,\n",
       "  3963,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  45093,\n",
       "  3963,\n",
       "  108,\n",
       "  235309,\n",
       "  42446,\n",
       "  16960,\n",
       "  235307,\n",
       "  109,\n",
       "  235309,\n",
       "  143005,\n",
       "  235307,\n",
       "  108,\n",
       "  2169,\n",
       "  5528,\n",
       "  4202,\n",
       "  590,\n",
       "  791,\n",
       "  674,\n",
       "  235269,\n",
       "  573,\n",
       "  4433,\n",
       "  3963,\n",
       "  603,\n",
       "  3110,\n",
       "  235269,\n",
       "  139,\n",
       "  1175,\n",
       "  1634,\n",
       "  603,\n",
       "  8144,\n",
       "  235269,\n",
       "  573,\n",
       "  9010,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  235269,\n",
       "  573,\n",
       "  3118,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  9010,\n",
       "  3963,\n",
       "  235269,\n",
       "  573,\n",
       "  4433,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  3118,\n",
       "  3963,\n",
       "  235265,\n",
       "  108,\n",
       "  2926,\n",
       "  6789,\n",
       "  603,\n",
       "  577,\n",
       "  791,\n",
       "  674,\n",
       "  139,\n",
       "  1175,\n",
       "  9010,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  235269,\n",
       "  573,\n",
       "  3118,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  9010,\n",
       "  3963,\n",
       "  235269,\n",
       "  573,\n",
       "  4433,\n",
       "  3963,\n",
       "  603,\n",
       "  611,\n",
       "  573,\n",
       "  3037,\n",
       "  235265,\n",
       "  109,\n",
       "  2926,\n",
       "  1780,\n",
       "  603,\n",
       "  685,\n",
       "  6397,\n",
       "  235292,\n",
       "  235248,\n",
       "  109,\n",
       "  235309,\n",
       "  42446,\n",
       "  235307,\n",
       "  109],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [2,\n",
       "  235309,\n",
       "  42446,\n",
       "  235307,\n",
       "  108,\n",
       "  549,\n",
       "  8388,\n",
       "  573,\n",
       "  4433,\n",
       "  3963,\n",
       "  774,\n",
       "  611,\n",
       "  2267,\n",
       "  576,\n",
       "  573,\n",
       "  3118,\n",
       "  3963,\n",
       "  108,\n",
       "  1065,\n",
       "  1706,\n",
       "  573,\n",
       "  4433,\n",
       "  3963,\n",
       "  108,\n",
       "  235309,\n",
       "  42446,\n",
       "  16960,\n",
       "  235307]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text=tokenized_data['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text=tokenizer.decode(encoded_text,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
      "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
      "        I have the following restrictions on my actions:\n",
      "        I can only pick up or unstack one block at a time\n",
      "        I can only pick up or unstack a block if my hand is empty\n",
      "        I can only pick up a block if the block is on the table and the block is clear\n",
      "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
      "        Once I pick up or unstack a block, I am holding the block\n",
      "        I can only put down a block that I am holding\n",
      "        I can only stack a block on top of another block if I am holding the block being stacked\n",
      "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
      "        Once I put down or stack a block, my hand becomes empty\n",
      "        Once you stack a block on top of a second block, the second block is no longer clear\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
      "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "unstack the brown block from on top of the violet block\n",
      "put down the brown block\n",
      "pick up the violet block\n",
      "stack the violet block on top of the brown block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the violet block\n",
      "[PLAN END]\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
      "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
      "\n",
      "My plan is as follows: \n",
      "\n",
      "[PLAN]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"google/gemma-2-9b-it\",\n",
       "  \"architectures\": [\n",
       "    \"Gemma2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attn_logit_softcapping\": 50.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"cache_implementation\": \"hybrid\",\n",
       "  \"eos_token_id\": 1,\n",
       "  \"final_logit_softcapping\": 30.0,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_size\": 3584,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma2\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 42,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"query_pre_attn_scalar\": 256,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"sliding_window_size\": 4096,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.47.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planwllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
