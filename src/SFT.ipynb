{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma3ForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset\n",
    "import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "name='google/gemma-3-12b-it'\n",
    "cache_dir='/home/chawak/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36bdd233c224a80a97a479e5876272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Tokenize the dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split='val'\n",
    "n=4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1-1 Gemma-3 tokenizing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace dataset : DatasetDict({\n",
      "    val: Dataset({\n",
      "        features: ['prompt', 'gold_plan'],\n",
      "        num_rows: 483\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "import numpy as np\n",
    "\n",
    "#dataset_path=f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks'\n",
    "dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "data=data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan'])\n",
    "print(f'Loaded HuggingFace dataset : {data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token: <eos>\n",
      "EOS token ID: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"EOS token:\", tokenizer.eos_token)\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# train_text={'prompt':['I am cat'],\n",
    "#             'gold_plan':['I eat fish']}\n",
    "# val_text={'prompt':['I am dog'],\n",
    "#           'gold_plan':['I eat chicken']}\n",
    "# train_df=pd.DataFrame(data=train_text)\n",
    "# val_df=pd.DataFrame(data=val_text)\n",
    "# train_df.to_csv('../data/toy_train')\n",
    "# val_df.to_csv('../data/toy_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace dataset : DatasetDict({\n",
      "    val: Dataset({\n",
      "        features: ['prompt', 'gold_plan'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#load the TOY dataset\n",
    "# import numpy as np\n",
    "# split='val'\n",
    "# #dataset_path=f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks'\n",
    "# dataset_path=f'../data/toy_{split}'\n",
    "# data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "# data=data.remove_columns(['Unnamed: 0'])\n",
    "# print(f'Loaded HuggingFace dataset : {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function for tokenizing\n",
    "def tokenize_and_mask_function(examples):\n",
    "    #concatinate prompt and gold plan within our template\n",
    "    merged_inputs=[f\"{p[:-2]}{g[6:]}\" for p,g in zip(examples['prompt'],examples['gold_plan'])]\n",
    "\n",
    "    #tokenize concatinated input\n",
    "    tokenized=tokenizer(\n",
    "        merged_inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        padding_side='right',\n",
    "        max_length=1200\n",
    "    )\n",
    "\n",
    "    #tokenize ONLY the prompts and get their lengths\n",
    "    tokenized_prompts=tokenizer(\n",
    "        examples['prompt'],\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    #print(f'Tokenized prompts: {tokenized_prompts}')\n",
    "    prompt_lens = [len(ptoken) for ptoken in tokenized_prompts['input_ids']]\n",
    "    #print(\"Lengths of tokenized prompts\", prompt_lens)\n",
    "    \n",
    "    #estimating input token sequence lengths\n",
    "    merged_input_lens=[len(merged) for merged in tokenized['input_ids']]\n",
    "    sorted_lens=sorted(merged_input_lens, reverse=True)\n",
    "    #print('Highest lengths of merged-input token sequence:',sorted_lens[:5])\n",
    "    #print(\"Number of prompts tokenized\",len(prompt_lens))\n",
    "    \n",
    "    #masking prompt tokens for the labels \n",
    "    labels=[]\n",
    "    for input_ids, prompt_length in zip(tokenized['input_ids'],prompt_lens):\n",
    "        label=input_ids.copy()\n",
    "        #mask prompt tokens as -100 & adjustment for prompt template\n",
    "        label[:prompt_length-4]=[-100]*prompt_length\n",
    "        label=label[:-4]\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized['labels']=labels\n",
    "\n",
    "    tokenized['input_ids'] = [np.array(ids, dtype=np.int64) for ids in tokenized['input_ids']]\n",
    "    tokenized['labels']=[np.array(labels, dtype=np.int64) for labels in tokenized['labels']]\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map tokenizer function to our dataset\n",
    "tokenized_data=data.map(tokenize_and_mask_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    val: Dataset({\n",
       "        features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 483\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "# data=load_dataset(\"csv\",data_files={split:dataset_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "# data=pd.read_csv(path)\n",
    "# data=data.drop(columns=['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=f'../data/{n}_blocks/SFT_contaminated_test_{n}_blocks_fullPlan'\n",
    "# cont=pd.read_csv(path)\n",
    "# cont=cont.drop(columns=['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "# cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.concat([data,cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_data=cont_data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "# cont_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 236777,\n",
       " 1006,\n",
       " 5662,\n",
       " 607,\n",
       " 496,\n",
       " 1076,\n",
       " 529,\n",
       " 11802,\n",
       " 1298,\n",
       " 564,\n",
       " 1202,\n",
       " 531,\n",
       " 28913,\n",
       " 506,\n",
       " 11802,\n",
       " 1131,\n",
       " 59556,\n",
       " 107,\n",
       " 144,\n",
       " 8291,\n",
       " 659,\n",
       " 506,\n",
       " 7419,\n",
       " 564,\n",
       " 740,\n",
       " 776,\n",
       " 236787,\n",
       " 24067,\n",
       " 872,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 1286,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 236764,\n",
       " 13499,\n",
       " 1679,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 28566,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 236761,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 735,\n",
       " 506,\n",
       " 2269,\n",
       " 14523,\n",
       " 580,\n",
       " 1041,\n",
       " 7419,\n",
       " 236787,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 4351,\n",
       " 872,\n",
       " 653,\n",
       " 723,\n",
       " 9607,\n",
       " 886,\n",
       " 3355,\n",
       " 657,\n",
       " 496,\n",
       " 990,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 4351,\n",
       " 872,\n",
       " 653,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 768,\n",
       " 1041,\n",
       " 1526,\n",
       " 563,\n",
       " 7738,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 4351,\n",
       " 872,\n",
       " 496,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 532,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 107,\n",
       " 144,\n",
       " 236776,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 815,\n",
       " 951,\n",
       " 1032,\n",
       " 11802,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 625,\n",
       " 532,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 711,\n",
       " 14917,\n",
       " 872,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 564,\n",
       " 1006,\n",
       " 723,\n",
       " 9607,\n",
       " 522,\n",
       " 691,\n",
       " 2126,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 1032,\n",
       " 3355,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 564,\n",
       " 1006,\n",
       " 723,\n",
       " 9607,\n",
       " 522,\n",
       " 563,\n",
       " 3582,\n",
       " 107,\n",
       " 144,\n",
       " 14946,\n",
       " 564,\n",
       " 4351,\n",
       " 872,\n",
       " 653,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 564,\n",
       " 1006,\n",
       " 7046,\n",
       " 506,\n",
       " 3355,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 2247,\n",
       " 1679,\n",
       " 496,\n",
       " 3355,\n",
       " 600,\n",
       " 564,\n",
       " 1006,\n",
       " 7046,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 564,\n",
       " 1006,\n",
       " 7046,\n",
       " 506,\n",
       " 3355,\n",
       " 1646,\n",
       " 45844,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 8990,\n",
       " 837,\n",
       " 564,\n",
       " 1006,\n",
       " 61573,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 107,\n",
       " 144,\n",
       " 14946,\n",
       " 564,\n",
       " 2247,\n",
       " 1679,\n",
       " 653,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 1041,\n",
       " 1526,\n",
       " 6775,\n",
       " 7738,\n",
       " 107,\n",
       " 144,\n",
       " 14946,\n",
       " 611,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 496,\n",
       " 1855,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 1855,\n",
       " 3355,\n",
       " 563,\n",
       " 951,\n",
       " 4890,\n",
       " 3582,\n",
       " 108,\n",
       " 236840,\n",
       " 196560,\n",
       " 236842,\n",
       " 107,\n",
       " 2205,\n",
       " 4068,\n",
       " 3439,\n",
       " 564,\n",
       " 735,\n",
       " 600,\n",
       " 236764,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 236764,\n",
       " 138,\n",
       " 1437,\n",
       " 1526,\n",
       " 563,\n",
       " 7738,\n",
       " 236764,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 236761,\n",
       " 107,\n",
       " 4754,\n",
       " 5671,\n",
       " 563,\n",
       " 531,\n",
       " 735,\n",
       " 600,\n",
       " 138,\n",
       " 1437,\n",
       " 2173,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 236761,\n",
       " 108,\n",
       " 4754,\n",
       " 2731,\n",
       " 563,\n",
       " 618,\n",
       " 5238,\n",
       " 236787,\n",
       " 108,\n",
       " 236840,\n",
       " 93868,\n",
       " 236842,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 236840,\n",
       " 93868,\n",
       " 21734,\n",
       " 236842,\n",
       " 108,\n",
       " 236840,\n",
       " 196560,\n",
       " 236842,\n",
       " 107,\n",
       " 2205,\n",
       " 4068,\n",
       " 3439,\n",
       " 564,\n",
       " 735,\n",
       " 600,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 236764,\n",
       " 138,\n",
       " 1437,\n",
       " 1526,\n",
       " 563,\n",
       " 7738,\n",
       " 236764,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 236761,\n",
       " 107,\n",
       " 4754,\n",
       " 5671,\n",
       " 563,\n",
       " 531,\n",
       " 735,\n",
       " 600,\n",
       " 138,\n",
       " 1437,\n",
       " 11167,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236761,\n",
       " 108,\n",
       " 4754,\n",
       " 2731,\n",
       " 563,\n",
       " 618,\n",
       " 5238,\n",
       " 236787,\n",
       " 236743,\n",
       " 108,\n",
       " 236840,\n",
       " 93868,\n",
       " 236842,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 236840,\n",
       " 93868,\n",
       " 21734,\n",
       " 236842,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ip=tokenized_data[split][0]['input_ids']\n",
    "encoded_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
      "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
      "        I have the following restrictions on my actions:\n",
      "        I can only pick up or unstack one block at a time\n",
      "        I can only pick up or unstack a block if my hand is empty\n",
      "        I can only pick up a block if the block is on the table and the block is clear\n",
      "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
      "        Once I pick up or unstack a block, I am holding the block\n",
      "        I can only put down a block that I am holding\n",
      "        I can only stack a block on top of another block if I am holding the block being stacked\n",
      "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
      "        Once I put down or stack a block, my hand becomes empty\n",
      "        Once you stack a block on top of a second block, the second block is no longer clear\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the purple block is clear,  the hand is empty, the white block is on the table, the teal block is on top of the white block, the orange block is on top of the teal block, the purple block is on top of the orange block.\n",
      "My goal is to have that  the white block is on the table, the purple block is on top of the white block, the orange block is on top of the purple block, the teal block is on top of the orange block.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "unstack the purple block from on top of the orange block\n",
      "put down the purple block\n",
      "unstack the orange block from on top of the teal block\n",
      "put down the orange block\n",
      "unstack the teal block from on top of the white block\n",
      "stack the teal block on top of the orange block\n",
      "pick up the purple block\n",
      "stack the purple block on top of the white block\n",
      "unstack the teal block from on top of the orange block\n",
      "put down the teal block\n",
      "pick up the orange block\n",
      "stack the orange block on top of the purple block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the orange block\n",
      "[PLAN END]\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the teal block is clear,  the hand is empty, the white block is on the table, the orange block is on top of the white block, the purple block is on top of the orange block, the teal block is on top of the purple block.\n",
      "My goal is to have that  the orange block is on the table, the teal block is on top of the orange block, the white block is on top of the teal block, the purple block is on the table.\n",
      "\n",
      "My plan is as follows: \n",
      "\n",
      "[PLAN]\n",
      "unstack the teal block from on top of the purple block\n",
      "put down the teal block\n",
      "unstack the purple block from on top of the orange block\n",
      "put down the purple block\n",
      "unstack the orange block from on top of the white block\n",
      "put down the orange block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the orange block\n",
      "pick up the white block\n",
      "stack the white block on top of the teal block\n",
      "[PLAN END]<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "decoded_ip=tokenizer.decode(encoded_ip)\n",
    "print(decoded_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 236840,\n",
       " 93868,\n",
       " 236842,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 16045,\n",
       " 3355,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 11167,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 2173,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 236840,\n",
       " 93868,\n",
       " 21734,\n",
       " 236842,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_op=tokenized_data[split][0]['labels']\n",
    "encoded_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PLAN]\n",
      "unstack the teal block from on top of the purple block\n",
      "put down the teal block\n",
      "unstack the purple block from on top of the orange block\n",
      "put down the purple block\n",
      "unstack the orange block from on top of the white block\n",
      "put down the orange block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the orange block\n",
      "pick up the white block\n",
      "stack the white block on top of the teal block\n",
      "[PLAN END]<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "decoded_op=tokenizer.decode([token for token in encoded_op if token != -100])\n",
    "print(decoded_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08584dd2723a4a5f8c5757682ad41292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/483 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/toy_label_nopad\")\n",
    "tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/{n}_blocks/tokenized_dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1-2 Dummy data tokenizing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded labels : <bos>I am dogI eat chicken<eos>\n"
     ]
    }
   ],
   "source": [
    "op=[2, 236777,   1006,   4799, 236777,   9039,  12480,      1,   -100, -100]\n",
    "ip=[     2, 236777,   1006,   4799, 236777,   9039,  12480,      1,      0,\n",
    "             0]\n",
    "decoded_op=tokenizer.decode([token for token in op if token != -100])\n",
    "print(f'Decoded labels : {decoded_op}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying encoded file storage\n",
    "from datasets import Dataset\n",
    "split='train'\n",
    "n=3\n",
    "# /srv/chawak/planning-with-llms/data/3_blocks/tokenized_dataset/test/\n",
    "ds= Dataset.from_file(f'/srv/chawak/planning-with-llms/data/{n}_blocks/tokenized_dataset/{split}/data-00000-of-00001.arrow')\n",
    "#ds= Dataset.from_file(f'/srv/chawak/planning-with-llms/data/toy_label_nopad/{split}/data-00000-of-00001.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 236777,\n",
       " 1006,\n",
       " 5662,\n",
       " 607,\n",
       " 496,\n",
       " 1076,\n",
       " 529,\n",
       " 11802,\n",
       " 1298,\n",
       " 564,\n",
       " 1202,\n",
       " 531,\n",
       " 28913,\n",
       " 506,\n",
       " 11802,\n",
       " 1131,\n",
       " 59556,\n",
       " 107,\n",
       " 144,\n",
       " 8291,\n",
       " 659,\n",
       " 506,\n",
       " 7419,\n",
       " 564,\n",
       " 740,\n",
       " 776,\n",
       " 236787,\n",
       " 24067,\n",
       " 872,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 1286,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 236764,\n",
       " 13499,\n",
       " 1679,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 28566,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 236761,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 735,\n",
       " 506,\n",
       " 2269,\n",
       " 14523,\n",
       " 580,\n",
       " 1041,\n",
       " 7419,\n",
       " 236787,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 4351,\n",
       " 872,\n",
       " 653,\n",
       " 723,\n",
       " 9607,\n",
       " 886,\n",
       " 3355,\n",
       " 657,\n",
       " 496,\n",
       " 990,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 4351,\n",
       " 872,\n",
       " 653,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 768,\n",
       " 1041,\n",
       " 1526,\n",
       " 563,\n",
       " 7738,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 4351,\n",
       " 872,\n",
       " 496,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 532,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 107,\n",
       " 144,\n",
       " 236776,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 815,\n",
       " 951,\n",
       " 1032,\n",
       " 11802,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 625,\n",
       " 532,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 711,\n",
       " 14917,\n",
       " 872,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 564,\n",
       " 1006,\n",
       " 723,\n",
       " 9607,\n",
       " 522,\n",
       " 691,\n",
       " 2126,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 1032,\n",
       " 3355,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 564,\n",
       " 1006,\n",
       " 723,\n",
       " 9607,\n",
       " 522,\n",
       " 563,\n",
       " 3582,\n",
       " 107,\n",
       " 144,\n",
       " 14946,\n",
       " 564,\n",
       " 4351,\n",
       " 872,\n",
       " 653,\n",
       " 723,\n",
       " 9607,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 564,\n",
       " 1006,\n",
       " 7046,\n",
       " 506,\n",
       " 3355,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 2247,\n",
       " 1679,\n",
       " 496,\n",
       " 3355,\n",
       " 600,\n",
       " 564,\n",
       " 1006,\n",
       " 7046,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 564,\n",
       " 1006,\n",
       " 7046,\n",
       " 506,\n",
       " 3355,\n",
       " 1646,\n",
       " 45844,\n",
       " 107,\n",
       " 144,\n",
       " 236777,\n",
       " 740,\n",
       " 1186,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 2264,\n",
       " 3355,\n",
       " 768,\n",
       " 506,\n",
       " 3355,\n",
       " 8990,\n",
       " 837,\n",
       " 564,\n",
       " 1006,\n",
       " 61573,\n",
       " 506,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 107,\n",
       " 144,\n",
       " 14946,\n",
       " 564,\n",
       " 2247,\n",
       " 1679,\n",
       " 653,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 236764,\n",
       " 1041,\n",
       " 1526,\n",
       " 6775,\n",
       " 7738,\n",
       " 107,\n",
       " 144,\n",
       " 14946,\n",
       " 611,\n",
       " 10166,\n",
       " 496,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 496,\n",
       " 1855,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 1855,\n",
       " 3355,\n",
       " 563,\n",
       " 951,\n",
       " 4890,\n",
       " 3582,\n",
       " 108,\n",
       " 236840,\n",
       " 196560,\n",
       " 236842,\n",
       " 107,\n",
       " 2205,\n",
       " 4068,\n",
       " 3439,\n",
       " 564,\n",
       " 735,\n",
       " 600,\n",
       " 236764,\n",
       " 506,\n",
       " 8864,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 236764,\n",
       " 138,\n",
       " 1437,\n",
       " 1526,\n",
       " 563,\n",
       " 7738,\n",
       " 236764,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 8864,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236761,\n",
       " 107,\n",
       " 4754,\n",
       " 5671,\n",
       " 563,\n",
       " 531,\n",
       " 735,\n",
       " 600,\n",
       " 138,\n",
       " 1437,\n",
       " 8864,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 8864,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 236761,\n",
       " 108,\n",
       " 4754,\n",
       " 2731,\n",
       " 563,\n",
       " 618,\n",
       " 5238,\n",
       " 236787,\n",
       " 108,\n",
       " 236840,\n",
       " 93868,\n",
       " 236842,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 8864,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 8864,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 8864,\n",
       " 3355,\n",
       " 107,\n",
       " 20701,\n",
       " 872,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 107,\n",
       " 9607,\n",
       " 506,\n",
       " 98865,\n",
       " 3355,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 39261,\n",
       " 3355,\n",
       " 107,\n",
       " 236840,\n",
       " 93868,\n",
       " 21734,\n",
       " 236842,\n",
       " 108,\n",
       " 236840,\n",
       " 196560,\n",
       " 236842,\n",
       " 107,\n",
       " 2205,\n",
       " 4068,\n",
       " 3439,\n",
       " 564,\n",
       " 735,\n",
       " 600,\n",
       " 236764,\n",
       " 506,\n",
       " 3826,\n",
       " 3355,\n",
       " 563,\n",
       " 3582,\n",
       " 236764,\n",
       " 138,\n",
       " 1437,\n",
       " 1526,\n",
       " 563,\n",
       " 7738,\n",
       " 236764,\n",
       " 506,\n",
       " 9514,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 2604,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 9514,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 3826,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2604,\n",
       " 3355,\n",
       " 236761,\n",
       " 107,\n",
       " 4754,\n",
       " 5671,\n",
       " 563,\n",
       " 531,\n",
       " 735,\n",
       " 600,\n",
       " 138,\n",
       " 1437,\n",
       " 9514,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236764,\n",
       " 506,\n",
       " 2604,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 9514,\n",
       " 3355,\n",
       " 236764,\n",
       " 506,\n",
       " 3826,\n",
       " 3355,\n",
       " 563,\n",
       " 580,\n",
       " 506,\n",
       " 2633,\n",
       " 236761,\n",
       " 108,\n",
       " 4754,\n",
       " 2731,\n",
       " 563,\n",
       " 618,\n",
       " 5238,\n",
       " 236787,\n",
       " 236743,\n",
       " 108,\n",
       " 236840,\n",
       " 93868,\n",
       " 236842,\n",
       " 107,\n",
       " 602,\n",
       " 9607,\n",
       " 506,\n",
       " 3826,\n",
       " 3355,\n",
       " 699,\n",
       " 580,\n",
       " 1903,\n",
       " 529,\n",
       " 506,\n",
       " 2604,\n",
       " 3355,\n",
       " 107,\n",
       " 1000,\n",
       " 1679,\n",
       " 506,\n",
       " 3826,\n",
       " 3355,\n",
       " 107,\n",
       " 236840,\n",
       " 93868,\n",
       " 21734,\n",
       " 236842,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input=ds[0]['input_ids']\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'I am playing with a set of blocks where I need to arrange the blocks into stacks\\n        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\\n        I have the following restrictions on my actions:\\n        I can only pick up or unstack one block at a time\\n        I can only pick up or unstack a block if my hand is empty\\n        I can only pick up a block if the block is on the table and the block is clear\\n        A block is clear if the block has no other blocks on top of it and if the block is not picked up\\n        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\\n        I can only unstack a block from on top of another block if the block I am unstacking is clear\\n        Once I pick up or unstack a block, I am holding the block\\n        I can only put down a block that I am holding\\n        I can only stack a block on top of another block if I am holding the block being stacked\\n        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\\n        Once I put down or stack a block, my hand becomes empty\\n        Once you stack a block on top of a second block, the second block is no longer clear\\n\\n[STATEMENT]\\nAs initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\\nMy goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nunstack the brown block from on top of the violet block\\nput down the brown block\\npick up the violet block\\nstack the violet block on top of the brown block\\npick up the teal block\\nstack the teal block on top of the violet block\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\\nMy goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\\n\\nMy plan is as follows: \\n\\n[PLAN]\\n\\n',\n",
       " 'gold_plan': '[PLAN]\\nunstack the green block from on top of the red block\\nput down the green block\\n[PLAN END]',\n",
       " 'input_ids': [2,\n",
       "  236777,\n",
       "  1006,\n",
       "  5662,\n",
       "  607,\n",
       "  496,\n",
       "  1076,\n",
       "  529,\n",
       "  11802,\n",
       "  1298,\n",
       "  564,\n",
       "  1202,\n",
       "  531,\n",
       "  28913,\n",
       "  506,\n",
       "  11802,\n",
       "  1131,\n",
       "  59556,\n",
       "  107,\n",
       "  144,\n",
       "  8291,\n",
       "  659,\n",
       "  506,\n",
       "  7419,\n",
       "  564,\n",
       "  740,\n",
       "  776,\n",
       "  236787,\n",
       "  24067,\n",
       "  872,\n",
       "  496,\n",
       "  3355,\n",
       "  236764,\n",
       "  1286,\n",
       "  9607,\n",
       "  496,\n",
       "  3355,\n",
       "  699,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  2264,\n",
       "  3355,\n",
       "  236764,\n",
       "  13499,\n",
       "  1679,\n",
       "  496,\n",
       "  3355,\n",
       "  236764,\n",
       "  28566,\n",
       "  496,\n",
       "  3355,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  2264,\n",
       "  3355,\n",
       "  236761,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  735,\n",
       "  506,\n",
       "  2269,\n",
       "  14523,\n",
       "  580,\n",
       "  1041,\n",
       "  7419,\n",
       "  236787,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  4351,\n",
       "  872,\n",
       "  653,\n",
       "  723,\n",
       "  9607,\n",
       "  886,\n",
       "  3355,\n",
       "  657,\n",
       "  496,\n",
       "  990,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  4351,\n",
       "  872,\n",
       "  653,\n",
       "  723,\n",
       "  9607,\n",
       "  496,\n",
       "  3355,\n",
       "  768,\n",
       "  1041,\n",
       "  1526,\n",
       "  563,\n",
       "  7738,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  4351,\n",
       "  872,\n",
       "  496,\n",
       "  3355,\n",
       "  768,\n",
       "  506,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  532,\n",
       "  506,\n",
       "  3355,\n",
       "  563,\n",
       "  3582,\n",
       "  107,\n",
       "  144,\n",
       "  236776,\n",
       "  3355,\n",
       "  563,\n",
       "  3582,\n",
       "  768,\n",
       "  506,\n",
       "  3355,\n",
       "  815,\n",
       "  951,\n",
       "  1032,\n",
       "  11802,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  625,\n",
       "  532,\n",
       "  768,\n",
       "  506,\n",
       "  3355,\n",
       "  563,\n",
       "  711,\n",
       "  14917,\n",
       "  872,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  723,\n",
       "  9607,\n",
       "  496,\n",
       "  3355,\n",
       "  699,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  2264,\n",
       "  3355,\n",
       "  768,\n",
       "  506,\n",
       "  3355,\n",
       "  564,\n",
       "  1006,\n",
       "  723,\n",
       "  9607,\n",
       "  522,\n",
       "  691,\n",
       "  2126,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  1032,\n",
       "  3355,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  723,\n",
       "  9607,\n",
       "  496,\n",
       "  3355,\n",
       "  699,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  2264,\n",
       "  3355,\n",
       "  768,\n",
       "  506,\n",
       "  3355,\n",
       "  564,\n",
       "  1006,\n",
       "  723,\n",
       "  9607,\n",
       "  522,\n",
       "  563,\n",
       "  3582,\n",
       "  107,\n",
       "  144,\n",
       "  14946,\n",
       "  564,\n",
       "  4351,\n",
       "  872,\n",
       "  653,\n",
       "  723,\n",
       "  9607,\n",
       "  496,\n",
       "  3355,\n",
       "  236764,\n",
       "  564,\n",
       "  1006,\n",
       "  7046,\n",
       "  506,\n",
       "  3355,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  2247,\n",
       "  1679,\n",
       "  496,\n",
       "  3355,\n",
       "  600,\n",
       "  564,\n",
       "  1006,\n",
       "  7046,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  10166,\n",
       "  496,\n",
       "  3355,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  2264,\n",
       "  3355,\n",
       "  768,\n",
       "  564,\n",
       "  1006,\n",
       "  7046,\n",
       "  506,\n",
       "  3355,\n",
       "  1646,\n",
       "  45844,\n",
       "  107,\n",
       "  144,\n",
       "  236777,\n",
       "  740,\n",
       "  1186,\n",
       "  10166,\n",
       "  496,\n",
       "  3355,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  2264,\n",
       "  3355,\n",
       "  768,\n",
       "  506,\n",
       "  3355,\n",
       "  8990,\n",
       "  837,\n",
       "  564,\n",
       "  1006,\n",
       "  61573,\n",
       "  506,\n",
       "  3355,\n",
       "  563,\n",
       "  3582,\n",
       "  107,\n",
       "  144,\n",
       "  14946,\n",
       "  564,\n",
       "  2247,\n",
       "  1679,\n",
       "  653,\n",
       "  10166,\n",
       "  496,\n",
       "  3355,\n",
       "  236764,\n",
       "  1041,\n",
       "  1526,\n",
       "  6775,\n",
       "  7738,\n",
       "  107,\n",
       "  144,\n",
       "  14946,\n",
       "  611,\n",
       "  10166,\n",
       "  496,\n",
       "  3355,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  496,\n",
       "  1855,\n",
       "  3355,\n",
       "  236764,\n",
       "  506,\n",
       "  1855,\n",
       "  3355,\n",
       "  563,\n",
       "  951,\n",
       "  4890,\n",
       "  3582,\n",
       "  108,\n",
       "  236840,\n",
       "  196560,\n",
       "  236842,\n",
       "  107,\n",
       "  2205,\n",
       "  4068,\n",
       "  3439,\n",
       "  564,\n",
       "  735,\n",
       "  600,\n",
       "  236764,\n",
       "  506,\n",
       "  8864,\n",
       "  3355,\n",
       "  563,\n",
       "  3582,\n",
       "  236764,\n",
       "  506,\n",
       "  98865,\n",
       "  3355,\n",
       "  563,\n",
       "  3582,\n",
       "  236764,\n",
       "  138,\n",
       "  1437,\n",
       "  1526,\n",
       "  563,\n",
       "  7738,\n",
       "  236764,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  236764,\n",
       "  506,\n",
       "  8864,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  236764,\n",
       "  506,\n",
       "  98865,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  236761,\n",
       "  107,\n",
       "  4754,\n",
       "  5671,\n",
       "  563,\n",
       "  531,\n",
       "  735,\n",
       "  600,\n",
       "  138,\n",
       "  1437,\n",
       "  8864,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  236764,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  8864,\n",
       "  3355,\n",
       "  236764,\n",
       "  506,\n",
       "  98865,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  236761,\n",
       "  108,\n",
       "  4754,\n",
       "  2731,\n",
       "  563,\n",
       "  618,\n",
       "  5238,\n",
       "  236787,\n",
       "  108,\n",
       "  236840,\n",
       "  93868,\n",
       "  236842,\n",
       "  107,\n",
       "  602,\n",
       "  9607,\n",
       "  506,\n",
       "  8864,\n",
       "  3355,\n",
       "  699,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  107,\n",
       "  1000,\n",
       "  1679,\n",
       "  506,\n",
       "  8864,\n",
       "  3355,\n",
       "  107,\n",
       "  20701,\n",
       "  872,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  107,\n",
       "  9607,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  8864,\n",
       "  3355,\n",
       "  107,\n",
       "  20701,\n",
       "  872,\n",
       "  506,\n",
       "  98865,\n",
       "  3355,\n",
       "  107,\n",
       "  9607,\n",
       "  506,\n",
       "  98865,\n",
       "  3355,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  39261,\n",
       "  3355,\n",
       "  107,\n",
       "  236840,\n",
       "  93868,\n",
       "  21734,\n",
       "  236842,\n",
       "  108,\n",
       "  236840,\n",
       "  196560,\n",
       "  236842,\n",
       "  107,\n",
       "  2205,\n",
       "  4068,\n",
       "  3439,\n",
       "  564,\n",
       "  735,\n",
       "  600,\n",
       "  236764,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  563,\n",
       "  3582,\n",
       "  236764,\n",
       "  138,\n",
       "  1437,\n",
       "  1526,\n",
       "  563,\n",
       "  7738,\n",
       "  236764,\n",
       "  506,\n",
       "  9514,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  236764,\n",
       "  506,\n",
       "  2604,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  9514,\n",
       "  3355,\n",
       "  236764,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  2604,\n",
       "  3355,\n",
       "  236761,\n",
       "  107,\n",
       "  4754,\n",
       "  5671,\n",
       "  563,\n",
       "  531,\n",
       "  735,\n",
       "  600,\n",
       "  138,\n",
       "  1437,\n",
       "  9514,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  236764,\n",
       "  506,\n",
       "  2604,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  9514,\n",
       "  3355,\n",
       "  236764,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  563,\n",
       "  580,\n",
       "  506,\n",
       "  2633,\n",
       "  236761,\n",
       "  108,\n",
       "  4754,\n",
       "  2731,\n",
       "  563,\n",
       "  618,\n",
       "  5238,\n",
       "  236787,\n",
       "  236743,\n",
       "  108,\n",
       "  236840,\n",
       "  93868,\n",
       "  236842,\n",
       "  107,\n",
       "  602,\n",
       "  9607,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  699,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  2604,\n",
       "  3355,\n",
       "  107,\n",
       "  1000,\n",
       "  1679,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  107,\n",
       "  236840,\n",
       "  93868,\n",
       "  21734,\n",
       "  236842,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  236840,\n",
       "  93868,\n",
       "  236842,\n",
       "  107,\n",
       "  602,\n",
       "  9607,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  699,\n",
       "  580,\n",
       "  1903,\n",
       "  529,\n",
       "  506,\n",
       "  2604,\n",
       "  3355,\n",
       "  107,\n",
       "  1000,\n",
       "  1679,\n",
       "  506,\n",
       "  3826,\n",
       "  3355,\n",
       "  107,\n",
       "  236840,\n",
       "  93868,\n",
       "  21734,\n",
       "  236842,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  ...]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels=ds[0]['labels']\n",
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>I am playing with a set of blocks where I need to arrange the blocks into stacks\\n        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\\n        I have the following restrictions on my actions:\\n        I can only pick up or unstack one block at a time\\n        I can only pick up or unstack a block if my hand is empty\\n        I can only pick up a block if the block is on the table and the block is clear\\n        A block is clear if the block has no other blocks on top of it and if the block is not picked up\\n        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\\n        I can only unstack a block from on top of another block if the block I am unstacking is clear\\n        Once I pick up or unstack a block, I am holding the block\\n        I can only put down a block that I am holding\\n        I can only stack a block on top of another block if I am holding the block being stacked\\n        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\\n        Once I put down or stack a block, my hand becomes empty\\n        Once you stack a block on top of a second block, the second block is no longer clear\\n\\n[STATEMENT]\\nAs initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\\nMy goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nunstack the brown block from on top of the violet block\\nput down the brown block\\npick up the violet block\\nstack the violet block on top of the brown block\\npick up the teal block\\nstack the teal block on top of the violet block\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\\nMy goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\\n\\nMy plan is as follows: \\n\\n[PLAN]\\nunstack the green block from on top of the red block\\nput down the green block\\n[PLAN END]<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_ip=tokenizer.decode(encoded_input)\n",
    "decoded_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PLAN]\\nunstack the green block from on top of the red block\\nput down the green block\\n[PLAN END]<eos>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_op=tokenizer.decode([token for token in encoded_labels if token != -100])\n",
    "decoded_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', Gemma3ForCausalLM(\n",
      "  (model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "          (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "          (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3840, out_features=262208, bias=False)\n",
      "))\n",
      "('model', Gemma3TextModel(\n",
      "  (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-47): 48 x Gemma3DecoderLayer(\n",
      "      (self_attn): Gemma3Attention(\n",
      "        (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "        (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Gemma3MLP(\n",
      "        (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "        (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "        (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "      (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "      (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "      (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (rotary_emb): Gemma3RotaryEmbedding()\n",
      "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "))\n",
      "('model.embed_tokens', Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0))\n",
      "('model.layers', ModuleList(\n",
      "  (0-47): 48 x Gemma3DecoderLayer(\n",
      "    (self_attn): Gemma3Attention(\n",
      "      (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "      (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    )\n",
      "    (mlp): Gemma3MLP(\n",
      "      (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "      (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "      (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "      (act_fn): PytorchGELUTanh()\n",
      "    )\n",
      "    (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "    (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "    (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "    (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  )\n",
      "))\n",
      "('model.layers.0', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.0.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.0.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.0.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.0.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.0.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.0.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.0.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.0.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.0.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.0.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.0.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.0.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.0.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.0.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.0.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.0.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.1', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.1.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.1.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.1.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.1.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.1.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.1.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.1.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.1.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.1.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.1.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.1.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.1.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.1.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.1.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.1.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.1.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.2', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.2.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.2.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.2.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.2.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.2.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.2.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.2.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.2.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.2.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.2.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.2.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.2.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.2.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.2.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.2.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.2.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.3', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.3.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.3.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.3.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.3.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.3.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.3.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.3.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.3.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.3.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.3.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.3.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.3.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.3.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.3.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.3.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.3.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.4', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.4.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.4.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.4.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.4.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.4.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.4.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.4.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.4.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.4.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.4.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.4.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.4.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.4.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.4.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.4.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.4.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.5', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.5.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.5.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.5.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.5.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.5.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.5.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.5.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.5.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.5.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.5.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.5.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.5.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.5.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.5.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.5.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.5.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.6', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.6.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.6.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.6.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.6.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.6.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.6.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.6.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.6.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.6.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.6.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.6.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.6.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.6.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.6.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.6.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.6.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.7', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.7.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.7.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.7.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.7.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.7.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.7.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.7.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.7.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.7.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.7.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.7.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.7.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.7.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.7.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.7.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.7.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.8', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.8.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.8.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.8.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.8.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.8.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.8.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.8.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.8.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.8.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.8.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.8.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.8.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.8.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.8.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.8.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.8.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.9', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.9.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.9.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.9.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.9.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.9.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.9.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.9.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.9.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.9.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.9.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.9.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.9.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.9.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.9.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.9.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.9.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.10', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.10.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.10.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.10.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.10.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.10.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.10.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.10.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.10.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.10.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.10.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.10.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.10.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.10.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.10.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.10.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.10.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.11', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.11.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.11.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.11.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.11.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.11.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.11.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.11.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.11.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.11.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.11.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.11.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.11.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.11.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.11.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.11.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.11.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.12', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.12.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.12.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.12.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.12.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.12.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.12.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.12.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.12.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.12.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.12.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.12.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.12.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.12.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.12.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.12.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.12.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.13', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.13.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.13.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.13.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.13.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.13.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.13.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.13.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.13.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.13.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.13.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.13.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.13.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.13.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.13.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.13.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.13.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.14', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.14.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.14.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.14.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.14.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.14.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.14.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.14.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.14.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.14.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.14.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.14.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.14.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.14.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.14.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.14.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.14.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.15', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.15.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.15.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.15.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.15.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.15.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.15.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.15.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.15.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.15.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.15.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.15.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.15.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.15.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.15.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.15.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.15.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.16', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.16.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.16.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.16.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.16.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.16.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.16.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.16.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.16.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.16.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.16.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.16.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.16.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.16.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.16.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.16.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.16.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.17', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.17.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.17.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.17.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.17.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.17.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.17.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.17.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.17.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.17.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.17.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.17.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.17.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.17.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.17.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.17.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.17.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.18', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.18.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.18.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.18.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.18.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.18.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.18.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.18.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.18.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.18.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.18.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.18.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.18.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.18.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.18.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.18.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.18.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.19', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.19.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.19.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.19.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.19.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.19.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.19.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.19.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.19.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.19.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.19.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.19.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.19.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.19.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.19.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.19.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.19.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.20', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.20.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.20.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.20.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.20.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.20.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.20.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.20.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.20.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.20.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.20.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.20.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.20.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.20.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.20.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.20.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.20.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.21', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.21.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.21.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.21.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.21.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.21.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.21.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.21.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.21.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.21.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.21.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.21.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.21.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.21.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.21.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.21.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.21.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.22', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.22.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.22.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.22.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.22.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.22.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.22.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.22.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.22.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.22.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.22.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.22.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.22.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.22.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.22.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.22.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.22.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.23', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.23.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.23.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.23.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.23.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.23.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.23.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.23.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.23.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.23.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.23.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.23.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.23.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.23.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.23.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.23.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.23.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.24', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.24.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.24.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.24.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.24.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.24.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.24.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.24.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.24.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.24.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.24.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.24.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.24.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.24.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.24.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.24.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.24.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.25', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.25.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.25.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.25.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.25.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.25.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.25.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.25.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.25.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.25.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.25.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.25.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.25.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.25.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.25.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.25.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.25.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.26', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.26.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.26.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.26.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.26.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.26.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.26.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.26.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.26.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.26.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.26.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.26.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.26.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.26.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.26.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.26.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.26.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.27', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.27.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.27.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.27.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.27.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.27.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.27.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.27.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.27.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.27.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.27.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.27.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.27.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.27.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.27.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.27.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.27.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.28', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.28.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.28.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.28.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.28.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.28.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.28.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.28.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.28.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.28.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.28.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.28.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.28.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.28.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.28.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.28.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.28.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.29', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.29.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.29.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.29.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.29.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.29.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.29.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.29.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.29.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.29.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.29.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.29.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.29.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.29.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.29.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.29.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.29.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.30', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.30.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.30.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.30.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.30.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.30.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.30.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.30.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.30.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.30.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.30.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.30.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.30.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.30.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.30.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.30.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.30.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.31', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.31.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.31.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.31.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.31.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.31.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.31.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.31.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.31.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.31.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.31.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.31.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.31.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.31.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.31.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.31.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.31.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.32', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.32.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.32.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.32.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.32.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.32.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.32.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.32.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.32.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.32.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.32.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.32.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.32.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.32.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.32.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.32.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.32.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.33', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.33.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.33.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.33.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.33.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.33.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.33.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.33.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.33.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.33.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.33.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.33.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.33.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.33.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.33.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.33.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.33.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.34', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.34.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.34.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.34.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.34.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.34.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.34.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.34.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.34.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.34.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.34.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.34.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.34.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.34.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.34.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.34.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.34.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.35', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.35.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.35.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.35.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.35.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.35.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.35.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.35.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.35.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.35.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.35.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.35.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.35.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.35.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.35.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.35.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.35.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.36', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.36.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.36.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.36.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.36.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.36.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.36.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.36.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.36.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.36.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.36.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.36.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.36.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.36.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.36.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.36.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.36.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.37', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.37.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.37.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.37.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.37.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.37.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.37.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.37.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.37.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.37.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.37.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.37.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.37.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.37.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.37.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.37.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.37.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.38', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.38.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.38.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.38.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.38.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.38.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.38.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.38.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.38.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.38.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.38.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.38.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.38.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.38.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.38.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.38.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.38.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.39', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.39.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.39.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.39.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.39.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.39.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.39.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.39.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.39.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.39.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.39.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.39.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.39.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.39.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.39.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.39.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.39.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.40', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.40.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.40.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.40.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.40.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.40.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.40.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.40.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.40.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.40.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.40.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.40.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.40.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.40.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.40.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.40.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.40.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.41', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.41.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.41.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.41.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.41.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.41.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.41.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.41.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.41.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.41.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.41.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.41.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.41.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.41.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.41.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.41.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.41.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.42', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.42.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.42.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.42.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.42.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.42.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.42.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.42.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.42.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.42.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.42.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.42.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.42.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.42.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.42.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.42.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.42.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.43', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.43.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.43.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.43.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.43.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.43.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.43.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.43.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.43.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.43.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.43.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.43.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.43.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.43.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.43.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.43.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.43.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.44', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.44.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.44.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.44.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.44.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.44.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.44.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.44.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.44.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.44.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.44.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.44.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.44.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.44.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.44.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.44.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.44.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.45', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.45.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.45.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.45.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.45.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.45.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.45.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.45.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.45.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.45.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.45.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.45.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.45.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.45.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.45.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.45.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.45.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.46', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.46.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.46.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.46.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.46.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.46.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.46.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.46.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.46.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.46.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.46.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.46.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.46.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.46.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.46.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.46.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.46.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.47', Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "    (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
      "))\n",
      "('model.layers.47.self_attn', Gemma3Attention(\n",
      "  (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "))\n",
      "('model.layers.47.self_attn.q_proj', Linear(in_features=3840, out_features=4096, bias=False))\n",
      "('model.layers.47.self_attn.k_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.47.self_attn.v_proj', Linear(in_features=3840, out_features=2048, bias=False))\n",
      "('model.layers.47.self_attn.o_proj', Linear(in_features=4096, out_features=3840, bias=False))\n",
      "('model.layers.47.self_attn.q_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.47.self_attn.k_norm', Gemma3RMSNorm((256,), eps=1e-06))\n",
      "('model.layers.47.mlp', Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
      "  (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      "))\n",
      "('model.layers.47.mlp.gate_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.47.mlp.up_proj', Linear(in_features=3840, out_features=15360, bias=False))\n",
      "('model.layers.47.mlp.down_proj', Linear(in_features=15360, out_features=3840, bias=False))\n",
      "('model.layers.47.mlp.act_fn', PytorchGELUTanh())\n",
      "('model.layers.47.input_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.47.post_attention_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.47.pre_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.layers.47.post_feedforward_layernorm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.norm', Gemma3RMSNorm((3840,), eps=1e-06))\n",
      "('model.rotary_emb', Gemma3RotaryEmbedding())\n",
      "('model.rotary_emb_local', Gemma3RotaryEmbedding())\n",
      "('lm_head', Linear(in_features=3840, out_features=262208, bias=False))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for name in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define hyperparamters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import TrainingArguments, AdamW\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m data_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../data/\u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m_blocks/tokenized_dataset/\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m train_data\u001b[39m=\u001b[39mload_from_disk(data_path)\n\u001b[1;32m      6\u001b[0m train_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_disk' is not defined"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "n=3\n",
    "split='train'\n",
    "data_path=f'../data/{n}_blocks/tokenized_dataset/{split}'\n",
    "train_data=load_from_disk(data_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 15\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "n=3\n",
    "split='val'\n",
    "data_path=f'../data/{n}_blocks/tokenized_dataset/{split}'\n",
    "eval_data=load_from_disk(data_path)\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='../results/SFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#load model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer\u001b[39m=\u001b[39mAutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(cfg[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m],cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[1;32m      3\u001b[0m model\u001b[39m=\u001b[39mAutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     cfg[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m      6\u001b[0m     device_map\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdevice_map\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(cfg['model']['name'],cache_dir=cache_dir)\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    cfg['model']['name'],\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=cfg['model']['device_map'],\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=Dataset.from_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get config file\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg=yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LORA config \n",
    "peft_args=LoraConfig(\n",
    "    r=cfg['peft']['r'],\n",
    "    lora_alpha=cfg['peft']['lora_alpha'],\n",
    "    lora_dropout=cfg['peft']['lora_dropout'],\n",
    "    task_type=cfg['peft']['task_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get LORA model\n",
    "lora_model=get_peft_model(model,peft_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_layers=lora_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fc3a9152f80>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args=SFTConfig(\n",
    "    output_dir=cfg['training']['output_dir']+f'/{n}_blocks',\n",
    "    num_train_epochs=cfg['training']['num_train_epochs'],\n",
    "    per_device_train_batch_size= cfg['training']['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps=cfg['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=cfg['training']['learning_rate'],\n",
    "    weight_decay=cfg['training']['weight_decay'],\n",
    "    warmup_ratio=cfg['training']['warmup_ratio'],\n",
    "    #adam_epsilon=cfg['training']['adam_epsilon'],\n",
    "    #optim=cfg['training']['optim'],\n",
    "    logging_steps=cfg['training']['logging_steps'],\n",
    "    save_steps=cfg['training']['save_steps'],\n",
    "    eval_steps=cfg['training']['eval_steps'],\n",
    "    evaluation_strategy=cfg[\"training\"][\"evaluation_strategy\"],\n",
    "    save_strategy=cfg[\"training\"][\"save_strategy\"],\n",
    "    fp16=cfg[\"training\"][\"fp16\"],\n",
    "    bf16=cfg[\"training\"][\"bf16\"],\n",
    "    report_to=cfg[\"training\"][\"report_to\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer args dictionary\n",
    "optimizer = {\n",
    "    'params': lora_layers,\n",
    "    'lr': float(cfg['training']['learning_rate']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_args,\n",
    "    optimizer_cls_and_kwargs=(AdamW,optimizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2165\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2166\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2167\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2169\u001b[0m     )\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[39m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mno_sync, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_samples) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2526\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   3653\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3654\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, num_items_in_batch\u001b[39m=\u001b[39;49mnum_items_in_batch)\n\u001b[1;32m   3656\u001b[0m \u001b[39mdel\u001b[39;00m inputs\n\u001b[1;32m   3657\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3658\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   3660\u001b[0m ):\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:558\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mCompute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    557\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 558\u001b[0m (loss, outputs) \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcompute_loss(\n\u001b[1;32m    559\u001b[0m     model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_items_in_batch\u001b[39m=\u001b[39;49mnum_items_in_batch\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    562\u001b[0m     \u001b[39m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[39m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inputs:\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/trainer.py:3708\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m         loss_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_items_in_batch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3707\u001b[0m     inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3708\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   3709\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3710\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3711\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/peft/peft_model.py:1756\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_peft_forward_hooks(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1755\u001b[0m         kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1756\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m   1757\u001b[0m             input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1758\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1759\u001b[0m             inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1760\u001b[0m             labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1761\u001b[0m             output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1762\u001b[0m             output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1763\u001b[0m             return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1764\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1765\u001b[0m         )\n\u001b[1;32m   1767\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1768\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1769\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:977\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    976\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    978\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    979\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    980\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    981\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    982\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    983\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    984\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    985\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    986\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    987\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    988\u001b[0m )\n\u001b[1;32m    990\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    991\u001b[0m \u001b[39m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:763\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    752\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    753\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    754\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m         cache_position,\n\u001b[1;32m    761\u001b[0m     )\n\u001b[1;32m    762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 763\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    764\u001b[0m         hidden_states,\n\u001b[1;32m    765\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    766\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    767\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    768\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    769\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    770\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    771\u001b[0m     )\n\u001b[1;32m    773\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:479\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    476\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    478\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    480\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    481\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    482\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    483\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    484\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    485\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    486\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    488\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    489\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:402\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     attention_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_attn_implementation\n\u001b[0;32m--> 402\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m GEMMA2_ATTENTION_FUNCTION[attention_type](\n\u001b[1;32m    403\u001b[0m     \u001b[39mself\u001b[39;49m, query_states, key_states, value_states, attention_mask, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    404\u001b[0m )\n\u001b[1;32m    406\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(bsz, q_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    407\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:196\u001b[0m, in \u001b[0;36meager_attention_forward\u001b[0;34m(config, query, key, value, mask, **_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attn_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(query\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    195\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(attn_weights, p\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mattention_dropout, training\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m--> 196\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attn_weights, value_states)\n\u001b[1;32m    197\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Infer on the trained model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='./results/SFT/3_blocks/checkpoint-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15674d0e02544537971c339f4e2fa380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config=PeftConfig.from_pretrained(cache_dir)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=PeftModel.from_pretrained(base_model,cache_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(prompt,return_tensors='pt').to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len=inputs['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    }
   ],
   "source": [
    "inputs=tokenizer(prompt, return_tensors='pt').to('cuda:3')\n",
    "#print(input_id.device_map)\n",
    "#print(model.device_map)\n",
    "\n",
    "response=model.generate(**inputs, max_new_tokens=32)\n",
    "#print(f'Response from LLM: {tokenizer.decode(response[0][input_len:],skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: [PLAN]\n",
      "unstack the brown block from on top of the teal block\n",
      "put down the brown block\n",
      "unstack the teal block from on top of the violet block\n",
      "put down the teal block\n",
      "[PLAN END]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs=model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0,\n",
    "    )\n",
    "print(f'Response from LLM: {tokenizer.decode(outputs[0][input_len:],skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
    "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
    "        I have the following restrictions on my actions:\n",
    "        I can only pick up or unstack one block at a time\n",
    "        I can only pick up or unstack a block if my hand is empty\n",
    "        I can only pick up a block if the block is on the table and the block is clear\n",
    "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
    "        Once I pick up or unstack a block, I am holding the block\n",
    "        I can only put down a block that I am holding\n",
    "        I can only stack a block on top of another block if I am holding the block being stacked\n",
    "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
    "        Once I put down or stack a block, my hand becomes empty\n",
    "        Once you stack a block on top of a second block, the second block is no longer clear\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the green block is clear,  the hand is empty, the red block is on the table, the pink block is on top of the red block, the green block is on top of the pink block.\n",
    "My goal is to have that  the green block is on the table, the red block is on top of the green block, the pink block is on top of the red block.\n",
    "\n",
    "My plan is as follows:\n",
    "\n",
    "[PLAN]\n",
    "unstack the green block from on top of the pink block\n",
    "put down the green block\n",
    "unstack the pink block from on top of the red block\n",
    "put down the pink block\n",
    "pick up the red block\n",
    "stack the red block on top of the green block\n",
    "pick up the pink block\n",
    "stack the pink block on top of the red block\n",
    "[PLAN END]\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the brown block is clear,  the hand is empty, the violet block is on the table, the teal block is on top of the violet block, the brown block is on top of the teal block.\n",
    "My goal is to have that  the violet block is on the table, the teal block is on the table, the brown block is on the table.\n",
    "\n",
    "My plan is as follows: \n",
    "\n",
    "[PLAN]\n",
    "Answer within [PLAN] [PLAN END] tags.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 99\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 235285,\n",
       " 1144,\n",
       " 6155,\n",
       " 675,\n",
       " 476,\n",
       " 1142,\n",
       " 576,\n",
       " 13854,\n",
       " 1570,\n",
       " 590,\n",
       " 1476,\n",
       " 577,\n",
       " 9900,\n",
       " 573,\n",
       " 13854,\n",
       " 1280,\n",
       " 63297,\n",
       " 108,\n",
       " 145,\n",
       " 4858,\n",
       " 708,\n",
       " 573,\n",
       " 8737,\n",
       " 590,\n",
       " 798,\n",
       " 749,\n",
       " 235292,\n",
       " 17350,\n",
       " 908,\n",
       " 476,\n",
       " 3963,\n",
       " 235269,\n",
       " 2132,\n",
       " 8388,\n",
       " 476,\n",
       " 3963,\n",
       " 774,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 2550,\n",
       " 3963,\n",
       " 235269,\n",
       " 13298,\n",
       " 1706,\n",
       " 476,\n",
       " 3963,\n",
       " 235269,\n",
       " 23850,\n",
       " 476,\n",
       " 3963,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 2550,\n",
       " 3963,\n",
       " 235265,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 791,\n",
       " 573,\n",
       " 2412,\n",
       " 16842,\n",
       " 611,\n",
       " 970,\n",
       " 8737,\n",
       " 235292,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 4788,\n",
       " 908,\n",
       " 689,\n",
       " 748,\n",
       " 8388,\n",
       " 974,\n",
       " 3963,\n",
       " 696,\n",
       " 476,\n",
       " 1069,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 4788,\n",
       " 908,\n",
       " 689,\n",
       " 748,\n",
       " 8388,\n",
       " 476,\n",
       " 3963,\n",
       " 1013,\n",
       " 970,\n",
       " 1634,\n",
       " 603,\n",
       " 8144,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 4788,\n",
       " 908,\n",
       " 476,\n",
       " 3963,\n",
       " 1013,\n",
       " 573,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 578,\n",
       " 573,\n",
       " 3963,\n",
       " 603,\n",
       " 3110,\n",
       " 108,\n",
       " 145,\n",
       " 235280,\n",
       " 3963,\n",
       " 603,\n",
       " 3110,\n",
       " 1013,\n",
       " 573,\n",
       " 3963,\n",
       " 919,\n",
       " 793,\n",
       " 1156,\n",
       " 13854,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 665,\n",
       " 578,\n",
       " 1013,\n",
       " 573,\n",
       " 3963,\n",
       " 603,\n",
       " 780,\n",
       " 15532,\n",
       " 908,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 748,\n",
       " 8388,\n",
       " 476,\n",
       " 3963,\n",
       " 774,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 2550,\n",
       " 3963,\n",
       " 1013,\n",
       " 573,\n",
       " 3963,\n",
       " 590,\n",
       " 1144,\n",
       " 748,\n",
       " 8388,\n",
       " 574,\n",
       " 729,\n",
       " 2277,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 1156,\n",
       " 3963,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 748,\n",
       " 8388,\n",
       " 476,\n",
       " 3963,\n",
       " 774,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 2550,\n",
       " 3963,\n",
       " 1013,\n",
       " 573,\n",
       " 3963,\n",
       " 590,\n",
       " 1144,\n",
       " 748,\n",
       " 8388,\n",
       " 574,\n",
       " 603,\n",
       " 3110,\n",
       " 108,\n",
       " 145,\n",
       " 14326,\n",
       " 590,\n",
       " 4788,\n",
       " 908,\n",
       " 689,\n",
       " 748,\n",
       " 8388,\n",
       " 476,\n",
       " 3963,\n",
       " 235269,\n",
       " 590,\n",
       " 1144,\n",
       " 8576,\n",
       " 573,\n",
       " 3963,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 2507,\n",
       " 1706,\n",
       " 476,\n",
       " 3963,\n",
       " 674,\n",
       " 590,\n",
       " 1144,\n",
       " 8576,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 13410,\n",
       " 476,\n",
       " 3963,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 2550,\n",
       " 3963,\n",
       " 1013,\n",
       " 590,\n",
       " 1144,\n",
       " 8576,\n",
       " 573,\n",
       " 3963,\n",
       " 1855,\n",
       " 56368,\n",
       " 108,\n",
       " 145,\n",
       " 235285,\n",
       " 798,\n",
       " 1297,\n",
       " 13410,\n",
       " 476,\n",
       " 3963,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 2550,\n",
       " 3963,\n",
       " 1013,\n",
       " 573,\n",
       " 3963,\n",
       " 10401,\n",
       " 948,\n",
       " 590,\n",
       " 1144,\n",
       " 87337,\n",
       " 573,\n",
       " 3963,\n",
       " 603,\n",
       " 3110,\n",
       " 108,\n",
       " 145,\n",
       " 14326,\n",
       " 590,\n",
       " 2507,\n",
       " 1706,\n",
       " 689,\n",
       " 13410,\n",
       " 476,\n",
       " 3963,\n",
       " 235269,\n",
       " 970,\n",
       " 1634,\n",
       " 8485,\n",
       " 8144,\n",
       " 108,\n",
       " 145,\n",
       " 14326,\n",
       " 692,\n",
       " 13410,\n",
       " 476,\n",
       " 3963,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 476,\n",
       " 2257,\n",
       " 3963,\n",
       " 235269,\n",
       " 573,\n",
       " 2257,\n",
       " 3963,\n",
       " 603,\n",
       " 793,\n",
       " 5543,\n",
       " 3110,\n",
       " 109,\n",
       " 235309,\n",
       " 143005,\n",
       " 235307,\n",
       " 108,\n",
       " 2169,\n",
       " 5528,\n",
       " 4202,\n",
       " 590,\n",
       " 791,\n",
       " 674,\n",
       " 235269,\n",
       " 573,\n",
       " 8426,\n",
       " 3963,\n",
       " 603,\n",
       " 3110,\n",
       " 235269,\n",
       " 573,\n",
       " 88735,\n",
       " 3963,\n",
       " 603,\n",
       " 3110,\n",
       " 235269,\n",
       " 139,\n",
       " 1175,\n",
       " 1634,\n",
       " 603,\n",
       " 8144,\n",
       " 235269,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 235269,\n",
       " 573,\n",
       " 8426,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 235269,\n",
       " 573,\n",
       " 88735,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 235265,\n",
       " 108,\n",
       " 2926,\n",
       " 6789,\n",
       " 603,\n",
       " 577,\n",
       " 791,\n",
       " 674,\n",
       " 139,\n",
       " 1175,\n",
       " 8426,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 235269,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 8426,\n",
       " 3963,\n",
       " 235269,\n",
       " 573,\n",
       " 88735,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 235265,\n",
       " 109,\n",
       " 2926,\n",
       " 1780,\n",
       " 603,\n",
       " 685,\n",
       " 6397,\n",
       " 235292,\n",
       " 109,\n",
       " 235309,\n",
       " 42446,\n",
       " 235307,\n",
       " 108,\n",
       " 549,\n",
       " 8388,\n",
       " 573,\n",
       " 8426,\n",
       " 3963,\n",
       " 774,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 108,\n",
       " 1065,\n",
       " 1706,\n",
       " 573,\n",
       " 8426,\n",
       " 3963,\n",
       " 108,\n",
       " 18075,\n",
       " 908,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 108,\n",
       " 8388,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 8426,\n",
       " 3963,\n",
       " 108,\n",
       " 18075,\n",
       " 908,\n",
       " 573,\n",
       " 88735,\n",
       " 3963,\n",
       " 108,\n",
       " 8388,\n",
       " 573,\n",
       " 88735,\n",
       " 3963,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 45093,\n",
       " 3963,\n",
       " 108,\n",
       " 235309,\n",
       " 42446,\n",
       " 16960,\n",
       " 235307,\n",
       " 109,\n",
       " 235309,\n",
       " 143005,\n",
       " 235307,\n",
       " 108,\n",
       " 2169,\n",
       " 5528,\n",
       " 4202,\n",
       " 590,\n",
       " 791,\n",
       " 674,\n",
       " 235269,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 603,\n",
       " 3110,\n",
       " 235269,\n",
       " 139,\n",
       " 1175,\n",
       " 1634,\n",
       " 603,\n",
       " 8144,\n",
       " 235269,\n",
       " 573,\n",
       " 9010,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 235269,\n",
       " 573,\n",
       " 3118,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 9010,\n",
       " 3963,\n",
       " 235269,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 3118,\n",
       " 3963,\n",
       " 235265,\n",
       " 108,\n",
       " 2926,\n",
       " 6789,\n",
       " 603,\n",
       " 577,\n",
       " 791,\n",
       " 674,\n",
       " 139,\n",
       " 1175,\n",
       " 9010,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 235269,\n",
       " 573,\n",
       " 3118,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 9010,\n",
       " 3963,\n",
       " 235269,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 603,\n",
       " 611,\n",
       " 573,\n",
       " 3037,\n",
       " 235265,\n",
       " 109,\n",
       " 2926,\n",
       " 1780,\n",
       " 603,\n",
       " 685,\n",
       " 6397,\n",
       " 235292,\n",
       " 235248,\n",
       " 109,\n",
       " 235309,\n",
       " 42446,\n",
       " 235307,\n",
       " 108,\n",
       " 549,\n",
       " 8388,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 774,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 3118,\n",
       " 3963,\n",
       " 108,\n",
       " 1065,\n",
       " 1706,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 108,\n",
       " 235309,\n",
       " 42446,\n",
       " 16960,\n",
       " 235307,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 235309,\n",
       " 42446,\n",
       " 235307,\n",
       " 108,\n",
       " 549,\n",
       " 8388,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 774,\n",
       " 611,\n",
       " 2267,\n",
       " 576,\n",
       " 573,\n",
       " 3118,\n",
       " 3963,\n",
       " 108,\n",
       " 1065,\n",
       " 1706,\n",
       " 573,\n",
       " 4433,\n",
       " 3963,\n",
       " 108,\n",
       " 235309,\n",
       " 42446,\n",
       " 16960,\n",
       " 235307,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iuniprocessor equity ln\n"
     ]
    }
   ],
   "source": [
    "de=tokenizer.decode([235309,\n",
    "  42446,\n",
    "  16960,\n",
    "  235307])\n",
    "print(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/chawak/envs/planwllm/bin/python\n"
     ]
    }
   ],
   "source": [
    "from datasets import Features,Sequence, Value\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "split='val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace dataset : DatasetDict({\n",
      "    val: Dataset({\n",
      "        features: ['init', 'goal', 'demo_init', 'demo_goal', 'demo_plan', 'prompt', 'gold_plan'],\n",
      "        num_rows: 483\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c608850db7d44b4934bc593583fb1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/483 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the dataset\n",
    "import numpy as np\n",
    "\n",
    "#dataset_path=f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks'\n",
    "dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "data=data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan'])\n",
    "print(f'Loaded HuggingFace dataset : {data}')\n",
    "\n",
    "#helper function for tokenizing\n",
    "def tokenize_and_mask_function(examples):\n",
    "    #concatinate prompt and gold plan within our template\n",
    "    merged_inputs=[f\"{p[:-2]}{g[6:]}\" for p,g in zip(examples['prompt'],examples['gold_plan'])]\n",
    "\n",
    "    #tokenize concatinated input\n",
    "    tokenized=tokenizer(\n",
    "        merged_inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        padding_side='right',\n",
    "        max_length=1200\n",
    "    )\n",
    "\n",
    "    #tokenize ONLY the prompts and get their lengths\n",
    "    tokenized_prompts=tokenizer(\n",
    "        examples['prompt'],\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    #print(f'Tokenized prompts: {tokenized_prompts}')\n",
    "    prompt_lens = [len(ptoken) for ptoken in tokenized_prompts['input_ids']]\n",
    "    #print(\"Lengths of tokenized prompts\", prompt_lens)\n",
    "    \n",
    "    #estimating input token sequence lengths\n",
    "    merged_input_lens=[len(merged) for merged in tokenized['input_ids']]\n",
    "    sorted_lens=sorted(merged_input_lens, reverse=True)\n",
    "    #print('Highest lengths of merged-input token sequence:',sorted_lens[:5])\n",
    "    #print(\"Number of prompts tokenized\",len(prompt_lens))\n",
    "    \n",
    "    #masking prompt tokens for the labels \n",
    "    labels=[]\n",
    "    for input_ids, prompt_length in zip(tokenized['input_ids'],prompt_lens):\n",
    "        label=input_ids.copy()\n",
    "        #mask prompt tokens as -100 & adjustment for prompt template\n",
    "        label[:prompt_length-4]=[-100]*prompt_length\n",
    "        label=label[:-4]\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized['labels']=labels\n",
    "\n",
    "    tokenized['input_ids'] = [np.array(ids, dtype=np.int64) for ids in tokenized['input_ids']]\n",
    "    tokenized['labels']=[np.array(labels, dtype=np.int64) for labels in tokenized['labels']]\n",
    "    return tokenized\n",
    "\n",
    "#map tokenizer function to our dataset\n",
    "tokenized_data=data.map(tokenize_and_mask_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    val: Dataset({\n",
       "        features: ['init', 'goal', 'demo_init', 'demo_goal', 'demo_plan', 'prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 483\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = Features({\n",
    "    \"prompt\": Value(\"string\"),\n",
    "    \"gold_plan\": Value(\"string\"),\n",
    "    \"input_ids\": Sequence(Value(\"int64\")),\n",
    "    \"attention_mask\": Sequence(Value(\"int64\")),\n",
    "    \"labels\": Sequence(Value(\"int64\")),\n",
    "})\n",
    "\n",
    "tokenized_data = tokenized_data.cast(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dictonary after tokenization: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 114\n",
      "    })\n",
      "})\n",
      "Length of encoded merged-text : 1200\n",
      "Length of decoded merged-text : 1200\n",
      "Decoded merged-text: <bos>I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
      "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
      "        I have the following restrictions on my actions:\n",
      "        I can only pick up or unstack one block at a time\n",
      "        I can only pick up or unstack a block if my hand is empty\n",
      "        I can only pick up a block if the block is on the table and the block is clear\n",
      "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
      "        Once I pick up or unstack a block, I am holding the block\n",
      "        I can only put down a block that I am holding\n",
      "        I can only stack a block on top of another block if I am holding the block being stacked\n",
      "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
      "        Once I put down or stack a block, my hand becomes empty\n",
      "        Once you stack a block on top of a second block, the second block is no longer clear\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
      "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "unstack the brown block from on top of the violet block\n",
      "put down the brown block\n",
      "pick up the violet block\n",
      "stack the violet block on top of the brown block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the violet block\n",
      "[PLAN END]\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
      "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
      "\n",
      "My plan is as follows: \n",
      "\n",
      "[PLAN]\n",
      "unstack the green block from on top of the red block\n",
      "put down the green block\n",
      "[PLAN END]<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Length of encoded labels : 1200\n",
      "Decoded labels: [PLAN]\n",
      "unstack the green block from on top of the red block\n",
      "put down the green block\n",
      "[PLAN END]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040fa7bb6903477c94e1fccb6d85e7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sanity checks on tokenized data\n",
    "print(f'Data dictonary after tokenization: {tokenized_data}')\n",
    "#decoding for merged-text\n",
    "encoded_text=tokenized_data[split][0]['input_ids']\n",
    "print(f'Length of encoded merged-text : {len(encoded_text)}')\n",
    "decoded_text=tokenizer.decode(encoded_text, skip_special_tokens=False)\n",
    "print(f'Length of decoded merged-text : {len(encoded_text)}')\n",
    "print(f'Decoded merged-text: {decoded_text}')\n",
    "#deocding for lables\n",
    "encoded_text=tokenized_data[split][0]['labels']\n",
    "print(f'Length of encoded labels : {len(encoded_text)}')\n",
    "decoded_text=tokenizer.decode(\n",
    "    [token_id for token_id in encoded_text if token_id!=-100],\n",
    "    skip_special_tokens=True)\n",
    "print(f'Decoded labels: {decoded_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4a789edde24c5caaf10428105ae852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#save to file\n",
    "tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/{n}_blocks/tokenized_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_data['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81b11c8e61a4e588dfb572ceb40f7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 114\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "split='val'\n",
    "data_path=f'../data/{n}_blocks/tokenized_dataset/{split}'\n",
    "train_data=load_from_disk(data_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
      "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
      "        I have the following restrictions on my actions:\n",
      "        I can only pick up or unstack one block at a time\n",
      "        I can only pick up or unstack a block if my hand is empty\n",
      "        I can only pick up a block if the block is on the table and the block is clear\n",
      "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
      "        Once I pick up or unstack a block, I am holding the block\n",
      "        I can only put down a block that I am holding\n",
      "        I can only stack a block on top of another block if I am holding the block being stacked\n",
      "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
      "        Once I put down or stack a block, my hand becomes empty\n",
      "        Once you stack a block on top of a second block, the second block is no longer clear\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
      "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "unstack the brown block from on top of the violet block\n",
      "put down the brown block\n",
      "pick up the violet block\n",
      "stack the violet block on top of the brown block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the violet block\n",
      "[PLAN END]\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
      "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
      "\n",
      "My plan is as follows: \n",
      "\n",
      "[PLAN]\n",
      "unstack the green block from on top of the red block\n",
      "put down the green block\n",
      "[PLAN END]<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_data[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
    "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
    "        I have the following restrictions on my actions:\n",
    "        I can only pick up or unstack one block at a time\n",
    "        I can only pick up or unstack a block if my hand is empty\n",
    "        I can only pick up a block if the block is on the table and the block is clear\n",
    "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
    "        Once I pick up or unstack a block, I am holding the block\n",
    "        I can only put down a block that I am holding\n",
    "        I can only stack a block on top of another block if I am holding the block being stacked\n",
    "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
    "        Once I put down or stack a block, my hand becomes empty\n",
    "        Once you stack a block on top of a second block, the second block is no longer clear\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
    "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
    "\n",
    "My plan is as follows:\n",
    "\n",
    "[PLAN]\n",
    "unstack the brown block from on top of the violet block\n",
    "put down the brown block\n",
    "pick up the violet block\n",
    "stack the violet block on top of the brown block\n",
    "pick up the teal block\n",
    "stack the teal block on top of the violet block\n",
    "[PLAN END]\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
    "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
    "\n",
    "My plan is as follows: \n",
    "[PLAN]\n",
    "\n",
    "Strategise at each step before you choose an action, then form a final plan. Answer within the [PLAN] [PLAN END] tags.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=train_data[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/generation/utils.py:2372: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Unsupported",
     "evalue": "Unexpected type in sourceless builder transformers.models.gemma3.configuration_gemma3.Gemma3TextConfig\n\nfrom user code:\n   File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/hooks.py\", line 170, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py\", line 1319, in forward\n    causal_mask = self._update_causal_mask(\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py\", line 1114, in _update_causal_mask\n    if self.config.text_config._attn_implementation == \"flash_attention_2\":\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 211, in __getattribute__\n    return super().__getattribute__(key)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: input_ids}\n\u001b[0;32m----> 2\u001b[0m response\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/generation/utils.py:2490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2482\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2483\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2484\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2485\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2486\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2487\u001b[0m     )\n\u001b[1;32m   2489\u001b[0m     \u001b[39m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2490\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2491\u001b[0m         input_ids,\n\u001b[1;32m   2492\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2493\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2494\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2495\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2496\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2497\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2498\u001b[0m     )\n\u001b[1;32m   2500\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2501\u001b[0m     \u001b[39m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2503\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2504\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   2505\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2506\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2507\u001b[0m     )\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/generation/utils.py:3453\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3451\u001b[0m     is_prefill \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3453\u001b[0m     outputs \u001b[39m=\u001b[39m model_forward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3455\u001b[0m \u001b[39m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3456\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3457\u001b[0m     outputs,\n\u001b[1;32m   3458\u001b[0m     model_kwargs,\n\u001b[1;32m   3459\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3460\u001b[0m )\n",
      "File \u001b[0;32m/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:659\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    658\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39mexcept\u001b[39;00m ShortenTraceback \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    661\u001b[0m     \u001b[39m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[1;32m    662\u001b[0m     \u001b[39m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mremove_dynamo_frames() \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m  \u001b[39m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n",
      "\u001b[0;31mUnsupported\u001b[0m: Unexpected type in sourceless builder transformers.models.gemma3.configuration_gemma3.Gemma3TextConfig\n\nfrom user code:\n   File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/accelerate/hooks.py\", line 170, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py\", line 1319, in forward\n    causal_mask = self._update_causal_mask(\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py\", line 1114, in _update_causal_mask\n    if self.config.text_config._attn_implementation == \"flash_attention_2\":\n  File \"/srv/chawak/envs/planwllm/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 211, in __getattribute__\n    return super().__getattribute__(key)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "inputs = {'input_ids': input_ids}\n",
    "response=model.generate(**inputs, max_new_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1200])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.memory.empty_cache() -> None>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "n=10\n",
    "for i in range(n):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load evaluation dataset\n",
    "import pandas as pd\n",
    "\n",
    "n=3\n",
    "split='train'\n",
    "data_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "eval_data=pd.read_csv(data_path)\n",
    "eval_data=eval_data.drop(columns=['Unnamed: 0'])\n",
    "eval_data=eval_data.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init                               (('pink', 'red', 'green'),)\n",
       "goal                             (('pink', 'red'), ('green',))\n",
       "demo_init    the brown block is clear, the teal block is cl...\n",
       "demo_goal     the brown block is on the table, the violet b...\n",
       "demo_plan    [PLAN]\\nunstack the brown block from on top of...\n",
       "prompt       I am playing with a set of blocks where I need...\n",
       "gold_plan    [PLAN]\\nunstack the green block from on top of...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''\n",
    "Whats the weather like today\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt in the chat-template:<bos><start_of_turn>user\n",
      "Whats the weather like today<end_of_turn>\n",
      "\n",
      "\n",
      "\n",
      ".......Querying LLM for a plan......... iteration: #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/chawak/envs/planwllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Response from LLM:\n",
      "Whats the weather like today\n",
      "model\n",
      "Okay, let's get you the weather! To give you the most accurate forecast, I need to know **your location**.\n",
      "\n",
      "However, I can give you a general idea based on some major locations:\n",
      "\n",
      "*   **New York City:**\n",
      "\n",
      "After EXTRCAT PLAN ACTIONS: False\n",
      "\n",
      "\n",
      ".......Querying LLM for a plan......... iteration: #1\n",
      "######################### Response from LLM:\n",
      "Whats the weather like today\n",
      "model\n",
      "Okay, let's get you the weather! To give you the most accurate forecast, I need to know **your location**.\n",
      "\n",
      "However, I can give you a general idea based on some major locations:\n",
      "\n",
      "*   **New York City:**\n",
      "\n",
      "After EXTRCAT PLAN ACTIONS: False\n",
      "\n",
      "\n",
      ".......Querying LLM for a plan......... iteration: #2\n",
      "######################### Response from LLM:\n",
      "Whats the weather like today\n",
      "model\n",
      "Okay, let's get you the weather! To give you the most accurate forecast, I need to know **your location**.\n",
      "\n",
      "However, I can give you a general overview for some major US cities as of 1:30 PM PST\n",
      "\n",
      "After EXTRCAT PLAN ACTIONS: False\n"
     ]
    }
   ],
   "source": [
    "#tags=\"Answer within the [PLAN] [PLAN END] tags.\"\n",
    "response=infer(model=peft_model,prompt=prompt,temp=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan='''[PLAN]\n",
    "1. Unstack brown from violet.\n",
    "2. Put down brown.\n",
    "3. Unstack violet from teal.\n",
    "4. Put down violet.\n",
    "5. Pick up teal.\n",
    "6. Stack teal on brown.\n",
    "[PLAN END]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After EXTRCAT PLAN ACTIONS: ['1. Unstack brown from violet.', '2. Put down brown.', '3. Unstack violet from teal.', '4. Put down violet.', '5. Pick up teal.', '6. Stack teal on brown.']\n",
      "\n",
      "\n",
      " Action:1. Unstack brown from violet. after seperators:['1', 'Unstack', 'brown', 'from', 'violet']\n",
      "\n",
      "\n",
      " Action: 1. Unstack brown from violet., predicate: 1, counter: 0\n",
      "\n",
      "\n",
      " Action: 1. Unstack brown from violet., predicate: unstack, counter: 1\n",
      "\n",
      "For predicate: Unstack the result is : ('unstack', 'brown', 'violet')\n",
      "\n",
      "\n",
      " Action:2. Put down brown. after seperators:['2', 'Put', 'down', 'brown']\n",
      "\n",
      "\n",
      " Action: 2. Put down brown., predicate: 2, counter: 0\n",
      "\n",
      "\n",
      " Action: 2. Put down brown., predicate: put, counter: 1\n",
      "\n",
      "\n",
      " Action:3. Unstack violet from teal. after seperators:['3', 'Unstack', 'violet', 'from', 'teal']\n",
      "\n",
      "\n",
      " Action: 3. Unstack violet from teal., predicate: 3, counter: 0\n",
      "\n",
      "\n",
      " Action: 3. Unstack violet from teal., predicate: unstack, counter: 1\n",
      "\n",
      "For predicate: Unstack the result is : ('unstack', 'violet', 'teal')\n",
      "\n",
      "\n",
      " Action:4. Put down violet. after seperators:['4', 'Put', 'down', 'violet']\n",
      "\n",
      "\n",
      " Action: 4. Put down violet., predicate: 4, counter: 0\n",
      "\n",
      "\n",
      " Action: 4. Put down violet., predicate: put, counter: 1\n",
      "\n",
      "\n",
      " Action:5. Pick up teal. after seperators:['5', 'Pick', 'up', 'teal']\n",
      "\n",
      "\n",
      " Action: 5. Pick up teal., predicate: 5, counter: 0\n",
      "\n",
      "\n",
      " Action: 5. Pick up teal., predicate: pick, counter: 1\n",
      "\n",
      "\n",
      " Action:6. Stack teal on brown. after seperators:['6', 'Stack', 'teal', 'on', 'brown']\n",
      "\n",
      "\n",
      " Action: 6. Stack teal on brown., predicate: 6, counter: 0\n",
      "\n",
      "\n",
      " Action: 6. Stack teal on brown., predicate: stack, counter: 1\n"
     ]
    }
   ],
   "source": [
    "parsed=llm_utils.parse_action_tuples(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unstack', 'brown', 'violet'),\n",
       " ('put-down', 'brown'),\n",
       " ('unstack', 'violet', 'teal'),\n",
       " ('put-down', 'violet'),\n",
       " ('pick-up', 'teal'),\n",
       " ('stack', 'teal', 'brown')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "for model_it in range(9,10):\n",
    "    print(model_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= llm_utils.load_tokenized_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 114\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 15\n",
       " }))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'gold_plan', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 114\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
      "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
      "        I have the following restrictions on my actions:\n",
      "        I can only pick up or unstack one block at a time\n",
      "        I can only pick up or unstack a block if my hand is empty\n",
      "        I can only pick up a block if the block is on the table and the block is clear\n",
      "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
      "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
      "        Once I pick up or unstack a block, I am holding the block\n",
      "        I can only put down a block that I am holding\n",
      "        I can only stack a block on top of another block if I am holding the block being stacked\n",
      "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
      "        Once I put down or stack a block, my hand becomes empty\n",
      "        Once you stack a block on top of a second block, the second block is no longer clear\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
      "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "unstack the brown block from on top of the violet block\n",
      "put down the brown block\n",
      "pick up the violet block\n",
      "stack the violet block on top of the brown block\n",
      "pick up the teal block\n",
      "stack the teal block on top of the violet block\n",
      "[PLAN END]\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
      "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
      "\n",
      "My plan is as follows: \n",
      "\n",
      "[PLAN]\n",
      "unstack the green block from on top of the red block\n",
      "put down the green block\n",
      "[PLAN END]<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ds[0]['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planwllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
