{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/user/planning-with-llms/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "#get config file\n",
    "with open(\"/home/user/planning-with-llms/src/rl/config.yaml\", \"r\") as f:\n",
    "    cfg=yaml.safe_load(f)\n",
    "\n",
    "peft_args=LoraConfig(\n",
    "    r=int(cfg['peft']['r']),\n",
    "    lora_alpha=int(cfg['peft']['lora_alpha']),\n",
    "    lora_dropout=float(cfg['peft']['lora_dropout']),\n",
    "    task_type=cfg['peft']['task_type'],\n",
    "    target_modules=list(cfg['peft']['target_modules']),\n",
    ")\n",
    "model=get_peft_model(model,peft_args)\n",
    "model.get_input_embeddings().weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define n,split\n",
    "n=3\n",
    "split='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=PEFT_GRPO_trainer.main(3,'train')\n",
    "data_dir=f\"/home/user/planning-with-llms/data/{n}_blocks\"\n",
    "data_path=f'{data_dir}/GRPO_systhink_tokenized_dataset/{split}'\n",
    "data=load_from_disk(data_path)\n",
    "sample_size=1\n",
    "data=data.select(range(sample_size))\n",
    "print(f\"Data-sample size is: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "think='''I am a blocksworld plan generator.\n",
    "I first think about the reasoning process in the mind and then provide the user with the plan.\n",
    "The reasoning process and plan are enclosed within <think> </think> and [PLAN] [PLAN END] tags, respectively,\n",
    "i.e., <think> reasoning process here </think> [PLAN] plan here [PLAN END].'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=data[0]['input_ids']\n",
    "prompt=think+data[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input,processor=llm_utils.get_tokenized_input(prompt=prompt,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_input = processor.decode(input, skip_special_tokens=True)\n",
    "# print(decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=llm_utils.query_local_model(\n",
    "    tokenized_input=tokenized_input,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE THIS: Matthias ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[236777,   1006,   5662,  93868, 236842,    108]\n",
    "tokens2=[     2,      2,    105,   2364,    107, 236777,   1006,    496,   3355,\n",
    "           1745,   3156,   2731,  17579, 236761,    107, 236777,   1171,   1751,\n",
    "           1003,    506,  28507,   1657,    528,    506,   3666,    532,   1299,\n",
    "           2847,    506,   2430,    607,    506,   2731, 236761,    107,    818,\n",
    "          28507,   1657,    532,   2731,    659,  35585,   2351,    655,  36345,\n",
    "         107068,  36345, 236813,    532,    870,  93868, 236842,    870,  93868,\n",
    "          21734, 236842,  16616, 236764,   6619, 236764,    107, 236747, 236761,\n",
    "         236744,   1126,    655,  36345, 236813,  28507,   1657,   1590,   1454,\n",
    "          36345, 236813,    870,  93868, 236842,   2731,   1590,    870,  93868,\n",
    "          21734,   1619,    109]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_inputs=[236777,   1006,    496,   3355,   1745,   3156,   2731,  17579, 236761,\n",
    "           107, 236777,   1171,   1751,   1003,    506,  28507,   1657,    528,\n",
    "           506,   3666,    532,   1299,   2847,    506,   2430,    607,    506,\n",
    "          2731, 236761,    107,    818,  28507,   1657,    532,   2731,    659,\n",
    "         35585,   2351,    655,  36345, 107068,  36345, 236813,    532,    870,\n",
    "         93868, 236842,    870,  93868,  21734, 236842,  16616, 236764,   6619,\n",
    "        236764,    107, 236747, 236761, 236744,   1126,    655,  36345, 236813,\n",
    "         28507,   1657,   1590,   1454,  36345, 236813,    870,  93868, 236842,\n",
    "          2731,   1590,    870,  93868,  21734,   1619,    107, 236777,   1006,\n",
    "          5662,    607,    496,   1076,    529,  11802,   1298,    564,   1202,\n",
    "           531,  28913,    506,  11802,   1131,  59556,    107,    144,   8291,\n",
    "           659,    506,   7419,    564,    740,    776, 236787,  24067,    872,\n",
    "           496,   3355, 236764,   1286,   9607,    496,   3355,    699,    580,\n",
    "          1903,    529,   2264,   3355, 236764,  13499,   1679,    496,   3355,\n",
    "        236764,  28566,    496,   3355,    580,   1903,    529,   2264,   3355,\n",
    "        236761,    107,    144, 236777,    735,    506,   2269,  14523,    580,\n",
    "          1041,   7419, 236787,    107,    144, 236777,    740,   1186,   4351,\n",
    "           872,    653,    723,   9607,    886,   3355,    657,    496,    990,\n",
    "           107,    144, 236777,    740,   1186,   4351,    872,    653,    723,\n",
    "          9607,    496,   3355,    768,   1041,   1526,    563,   7738,    107,\n",
    "           144, 236777,    740,   1186,   4351,    872,    496,   3355,    768,\n",
    "           506,   3355,    563,    580,    506,   2633,    532,    506,   3355,\n",
    "           563,   3582,    107,    144, 236776,   3355,    563,   3582,    768,\n",
    "           506,   3355,    815,    951,   1032,  11802,    580,   1903,    529,\n",
    "           625,    532,    768,    506,   3355,    563,    711,  14917,    872,\n",
    "           107,    144, 236777,    740,   1186,    723,   9607,    496,   3355,\n",
    "           699,    580,   1903,    529,   2264,   3355,    768,    506,   3355,\n",
    "           564,   1006,    723,   9607,    522,    691,   2126,    580,   1903,\n",
    "           529,    506,   1032,   3355,    107,    144, 236777,    740,   1186,\n",
    "           723,   9607,    496,   3355,    699,    580,   1903,    529,   2264,\n",
    "          3355,    768,    506,   3355,    564,   1006,    723,   9607,    522,\n",
    "           563,   3582,    107,    144,  14946,    564,   4351,    872,    653,\n",
    "           723,   9607,    496,   3355, 236764,    564,   1006,   7046,    506,\n",
    "          3355,    107,    144, 236777,    740,   1186,   2247,   1679,    496,\n",
    "          3355,    600,    564,   1006,   7046,    107,    144, 236777,    740,\n",
    "          1186,  10166,    496,   3355,    580,   1903,    529,   2264,   3355,\n",
    "           768,    564,   1006,   7046,    506,   3355,   1646,  45844,    107,\n",
    "           144, 236777,    740,   1186,  10166,    496,   3355,    580,   1903,\n",
    "           529,   2264,   3355,    768,    506,   3355,   8990,    837,    564,\n",
    "          1006,  61573,    506,   3355,    563,   3582,    107,    144,  14946,\n",
    "           564,   2247,   1679,    653,  10166,    496,   3355, 236764,   1041,\n",
    "          1526,   6775,   7738,    107,    144,  14946,    611,  10166,    496,\n",
    "          3355,    580,   1903,    529,    496,   1855,   3355, 236764,    506,\n",
    "          1855,   3355,    563,    951,   4890,   3582,    108, 236840, 196560,\n",
    "        236842,    107,   2205,   4068,   3439,    564,    735,    600, 236764,\n",
    "           506,   8864,   3355,    563,   3582, 236764,    506,  98865,   3355,\n",
    "           563,   3582, 236764,    138,   1437,   1526,    563,   7738, 236764,\n",
    "           506,  39261,   3355,    563,    580,    506,   2633, 236764,    506,\n",
    "          8864,   3355,    563,    580,   1903,    529,    506,  39261,   3355,\n",
    "        236764,    506,  98865,   3355,    563,    580,    506,   2633, 236761,\n",
    "           107,   4754,   5671,    563,    531,    735,    600,    138,   1437,\n",
    "          8864,   3355,    563,    580,    506,   2633, 236764,    506,  39261,\n",
    "          3355,    563,    580,   1903,    529,    506,   8864,   3355, 236764,\n",
    "           506,  98865,   3355,    563,    580,   1903,    529,    506,  39261,\n",
    "          3355, 236761,    108,   4754,   2731,    563,    618,   5238, 236787,\n",
    "           108, 236840,  93868, 236842,    107,    602,   9607,    506,   8864,\n",
    "          3355,    699,    580,   1903,    529,    506,  39261,   3355,    107,\n",
    "          1000,   1679,    506,   8864,   3355,    107,  20701,    872,    506,\n",
    "         39261,   3355,    107,   9607,    506,  39261,   3355,    580,   1903,\n",
    "           529,    506,   8864,   3355,    107,  20701,    872,    506,  98865,\n",
    "          3355,    107,   9607,    506,  98865,   3355,    580,   1903,    529,\n",
    "           506,  39261,   3355,    107, 236840,  93868,  21734, 236842,    108,\n",
    "        236840, 196560, 236842,    107,   2205,   4068,   3439,    564,    735,\n",
    "           600, 236764,    506,   3826,   3355,    563,   3582, 236764,    138,\n",
    "          1437,   1526,    563,   7738, 236764,    506,   9514,   3355,    563,\n",
    "           580,    506,   2633, 236764,    506,   2604,   3355,    563,    580,\n",
    "          1903,    529,    506,   9514,   3355, 236764,    506,   3826,   3355,\n",
    "           563,    580,   1903,    529,    506,   2604,   3355, 236761,    107,\n",
    "          4754,   5671,    563,    531,    735,    600,    138,   1437,   9514,\n",
    "          3355,    563,    580,    506,   2633, 236764,    506,   2604,   3355,\n",
    "           563,    580,   1903,    529,    506,   9514,   3355, 236764,    506,\n",
    "          3826,   3355,    563,    580,    506,   2633, 236761,    108,   4754,\n",
    "          2731,    563,    618,   5238, 236787, 236743,    108, 236840,  93868,\n",
    "        236842,    108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prompt_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_input = tokenizer.decode(prompt_inputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''\n",
    "I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
    "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
    "        I have the following restrictions on my actions:\n",
    "        I can only pick up or unstack one block at a time\n",
    "        I can only pick up or unstack a block if my hand is empty\n",
    "        I can only pick up a block if the block is on the table and the block is clear\n",
    "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
    "        Once I pick up or unstack a block, I am holding the block\n",
    "        I can only put down a block that I am holding\n",
    "        I can only stack a block on top of another block if I am holding the block being stacked\n",
    "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
    "        Once I put down or stack a block, my hand becomes empty\n",
    "        Once you stack a block on top of a second block, the second block is no longer clear\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
    "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
    "\n",
    "My plan is as follows:\n",
    "\n",
    "[PLAN]\n",
    "unstack the brown block from on top of the violet block\n",
    "put down the brown block\n",
    "pick up the violet block\n",
    "stack the violet block on top of the brown block\n",
    "pick up the teal block\n",
    "stack the teal block on top of the violet block\n",
    "[PLAN END]\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
    "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
    "\n",
    "My plan is as follows: \n",
    "\n",
    "[PLAN]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "think='''I am a blocksworld plan generator.\n",
    "I first think about the reasoning process in the mind and then provide the user with the plan.\n",
    "The reasoning process and plan are enclosed within <think> </think> and [PLAN] [PLAN END] tags, respectively,\n",
    "i.e., <think> reasoning process here </think> [PLAN] plan here [PLAN END].'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=think+prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on GSM8K ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_answer(answer)-> str:\n",
    "    number=answer.split(\"####\")[1].lstrip()\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gsm8k_questions(split=\"train\", use_one_shot=False) -> Dataset:\n",
    "    \"\"\"Loads and prepares the GSM8K dataset with optional one-shot prompting.\"\"\"\n",
    "    try:\n",
    "        data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "    def format_example(x):\n",
    "        prompt = [{'role': 'system', 'content': SYSTEM_PROMPT}]\n",
    "        if use_one_shot:\n",
    "            prompt.extend([\n",
    "                {'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n",
    "                {'role': 'assistant', 'content': XML_COT_FORMAT.format(\n",
    "                    reasoning=\"9 is divisible by 3 and 8 is divisible by 2, but 7 is prime.\",\n",
    "                    answer=\"7\"\n",
    "                )}\n",
    "            ])\n",
    "        prompt.append({'role': 'user', 'content': x['question']})\n",
    "        return {'prompt': prompt, 'answer': extract_numeric_answer(x['answer'])}\n",
    "\n",
    "    return data.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_gsm8k_questions(use_one_shot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip,processor=llm_utils.get_tokenized_input(dataset[0]['prompt'],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=llm_utils.query_local_model(ip,\n",
    "                               processor,\n",
    "                               model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_GSM8K_answer(text):\n",
    "    answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_GSM8K_answer(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select(range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "l = \"\\\\nÂ§18000\\\\n\"\n",
    "x = l.replace(\"\\\\n\", \"\").strip()      # removes literal \\n\n",
    "match = re.search(r\"\\d+\", x)\n",
    "if match:\n",
    "    print(match.group())    # Output: 18000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(dataset[0]['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
