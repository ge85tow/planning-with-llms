{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"unsloth==2025.3.19\" vllm wandb\n",
    "# ! pip uninstall -y typing_extensions &&  pip install typing_extensions==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/srv/chawak/planning-with-llms/src\")\n",
    "\n",
    "from shared import llm_utils\n",
    "from shared import unifiedplanning_blocksworld as bw\n",
    "from shared import prompts\n",
    "from shared import planbench as pb\n",
    "\n",
    "import random, os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='/srv/chawak/planning-with-llms/results/SFT'\n",
    "cpt=20430\n",
    "model_path=base_dir+f'/training/training_22-05/checkpoint-{cpt}'\n",
    "base_model='/home/chawak/huggingface/models--google--gemma-3-12b-it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FastLanguageModel.from_pretrained(\n",
    "    model_name=\"google/gemma-3-12b-it\",\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = FastLanguageModel.get_peft_model(\n",
    "    peft_model[0],\n",
    "    r=64,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Remove QKVO if out of memory\n",
    "    lora_alpha=64,\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_action_tuples(response):\n",
    "\n",
    "    #get action tuples\n",
    "    action_tuples=llm_utils.parse_action_tuples(response)\n",
    "    if not action_tuples:\n",
    "        print(f'\\n\\n Imparsable response')\n",
    "        return False\n",
    "    \n",
    "    return action_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response2plan(problem,init,goal,response,):\n",
    "    \n",
    "    model_plan=None\n",
    "    #create a blocksworld problem \n",
    "    init=prompts.parse_init(init)\n",
    "    goal=prompts.parse_goal(goal)\n",
    "    pb.parse_planbench_initial_condition(problem, init)\n",
    "    pb.parse_planbench_goal_state(problem, goal)\n",
    "    print(f'\\n\\nBlocksworld Problem Initial Values:{problem.initial_values}')\n",
    "    print(f'\\nBlocksworld Problem Goal State:{problem.goals}')\n",
    "\n",
    "    #get model plan\n",
    "    action_tuples=make_action_tuples(response)\n",
    "    model_plan=problem.GRPOcreate_plan_from_tuples(action_tuples)\n",
    "    \n",
    "    print(f\"Model responded with this plan: {model_plan}\")\n",
    "    return action_tuples,model_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_plan(problem,model_plan):\n",
    "    \n",
    "    #validate and apply    \n",
    "    simulation=problem.create_seq_simulation()\n",
    "    va_counter=0\n",
    "    valid_state,va_counter,distance2goal=problem.GRPO_check_and_apply(simulation,model_plan)\n",
    "    #check distance to goal for last valid state\n",
    "    d=problem.actions_to_goal(valid_state)\n",
    "    distance2goal.append(d)\n",
    "    \n",
    "    return valid_state,va_counter,distance2goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_proximity(distance2goal:list) -> list:\n",
    "    \n",
    "    scores=[]\n",
    "    \n",
    "    for idx,d in enumerate(distance2goal):\n",
    "        \n",
    "        if d == 0: #goal state reached\n",
    "            break\n",
    "\n",
    "        if(idx<=len(distance2goal)-2):\n",
    "            print(\"-\"*20,f\"IDX:{idx}\")\n",
    "            d_old=d\n",
    "            d_new=distance2goal[idx+1]\n",
    "            score=max(0,(d_old-d_new)*5)\n",
    "            scores.append(score)\n",
    " \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plan_len(plan):    \n",
    "    \n",
    "    plan_len=len(plan.split('\\n'))-2\n",
    "\n",
    "    return plan_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_proximity([4, 3, 2, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bonus rewards: correct termination, optimality\n",
    "def bonus_reward(problem,valid_state,plan_len,goldplanlen):\n",
    "    \n",
    "    score=0\n",
    "    #check if plan terminates to goal\n",
    "    if problem.terminate(valid_state):\n",
    "        score+=20 #20 for reaching goal\n",
    "        \n",
    "        #check if model plan matches the gold plan length\n",
    "        if plan_len==goldplanlen: score+=10\n",
    "\n",
    "        #check if model plan superceeds the gold plan length\n",
    "        if plan_len<goldplanlen: score+=15\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_plan_reward(problem, gold_plan):\n",
    "    \n",
    "    score=0\n",
    "\n",
    "    #get gold-plan plan object\n",
    "    action_tuples=make_action_tuples(gold_plan)\n",
    "    gold_plan_ob=problem.GRPOcreate_plan_from_tuples(action_tuples)\n",
    "    current_state,va_counter,distance2goal= apply_plan(problem,gold_plan_ob)\n",
    "\n",
    "    #score + 2 for each valid action\n",
    "    score+= va_counter*2\n",
    "    #score for when we are moving towards goal state\n",
    "    print(f\"GOLD-PLAN distance metric is: {distance2goal}\")\n",
    "    distance_scores=goal_proximity(distance2goal)\n",
    "    print(f\"GOLD-PLAN proximity scores are: {distance_scores}\")\n",
    "    score+=sum(distance_scores)\n",
    "    print(f\"GOLD-PLAN scores without bonus reward{score}\")\n",
    "\n",
    "    print('GOLD PLAN SCORE IS: ',score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_pattern=r\"<think>(.*?)<\\/think>.*?\\[PLAN\\](.*?)\\[PLAN END\\]\"\n",
    "format_pattern=re.compile(format_pattern, re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores structural adherence of response\n",
    "def format_reward(prompts,completions, **kwargs) -> list[float]:\n",
    "\n",
    "    responses= [completion[0][\"content\"] for completion in completions]\n",
    "    return [0.0 if not format_pattern.match(response) else 10.0 for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores plan correctness\n",
    "def response_score(response,init,goal,gold_plan):\n",
    "\n",
    "    print('-'*20,\"Entering response score\",'-'*20)\n",
    "    #define blocksworld problem \n",
    "    problem=bw.BlocksworldProblem()\n",
    "    score = 0\n",
    "\n",
    "    #VALID ACTION REWARD:\n",
    "    #extract valid actions from response\n",
    "    action_tuples,model_plan=response2plan(problem=problem,init=init,goal=goal,response=response)\n",
    "    current_state,valid_action_count,distance2goal=apply_plan(problem=problem,model_plan=model_plan)    \n",
    "    print(f'Distance 2 goal metric:{distance2goal}')\n",
    "    gold_plan_len=get_plan_len(gold_plan)\n",
    "    #limited to total number of actions possible, to avoid reward hacking\n",
    "    score+=min(gold_plan_len*2,valid_action_count*2)\n",
    "    print(f\"Valid actions score is: {score}\")\n",
    "\n",
    "    #PROXIMITY REWARD:\n",
    "    distance_scores=goal_proximity(distance2goal)\n",
    "    print(f\"Proximity scores are: {distance_scores}\")\n",
    "    score+=sum(distance_scores)\n",
    "\n",
    "    print(f\"Scores without bonus reward: {score}\")\n",
    "\n",
    "    #normalize model-plan's reward by the gold plan reward to 0-60 range\n",
    "    gold_plan_score=gold_plan_reward(problem=problem,gold_plan=gold_plan)\n",
    "    score=(score/gold_plan_score)*60\n",
    "\n",
    "    #bonus rewards for correct termination and optimality\n",
    "    score+=bonus_reward(problem,current_state,len(action_tuples),gold_plan_len)\n",
    "    print(f\"Scores with bonus reward: {score}\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=\"(('violet', 'teal', 'brown'),)\"\n",
    "goal=\"(('violet',), ('teal',), ('brown',))\"\n",
    "response='''[PLAN]\n",
    "unstack the brown block from on top of the teal block\n",
    "blah unstack blah\n",
    "put down the brown block\n",
    "[PLAN END]'''\n",
    "gold_plan='''[PLAN]\n",
    "unstack the brown block from on top of the teal block\n",
    "put down the brown block\n",
    "unstack the teal block from on top of the violet block\n",
    "put down the teal block\n",
    "[PLAN END]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_score(response=response,init=init,goal=goal,gold_plan=gold_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_reward(prompts,completions,init, goal, gold_plan):\n",
    "    responses= [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    scores=[]\n",
    "    for response in responses:\n",
    "        \n",
    "        score=0\n",
    "        score=response_score(response=response, init=init, goal=goal, gold_plan=gold_plan)\n",
    "        scores.append(score)            \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_pattern=r\".*?\\[PLAN\\](.*?)\\[PLAN END\\]\"\n",
    "format_pattern=re.compile(format_pattern, re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='''i have gibrish here\n",
    "[PLAN]\n",
    "unstack the brown block from on top of the teal block\n",
    "put down the brown block\n",
    "unstack the teal block from on top of the violet block\n",
    "put down the teal block\n",
    "[PLAN END]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.0 if not format_pattern.match(input) else 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def compute_mean_over_k_steps(metric_list,k_steps):\n",
    "\n",
    "    metric_chunks=[]\n",
    "\n",
    "    #split metrics list by step-size\n",
    "    for i in range(0, len(metric_list), k_steps):\n",
    "        metric_chunk=metric_list[i:i+k_steps]\n",
    "        metric_chunks.append(metric_chunk)\n",
    "\n",
    "    sums=[sum(metric_chunk) for metric_chunk in metric_chunks]\n",
    "    mean_values=[sum/k_steps for sum in sums]\n",
    "\n",
    "    print(mean_values)\n",
    "\n",
    "    return mean_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mean_over_k_steps(list(range(10)),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_pattern=r\"\\[PLAN\\](.*?)\\[PLAN END\\]\"\n",
    "format_pattern=re.compile(format_pattern, re.DOTALL)\n",
    "\n",
    "def format_reward(completions, **kwargs) -> list[float]:\n",
    "    \n",
    "    scores=[]\n",
    "    responses= [completion for completion in completions]\n",
    "    # print(f\"Example propmt is {prompts[0]}\")\n",
    "\n",
    "    for response in responses:\n",
    "\n",
    "        score=0\n",
    "        #hard format reward\n",
    "        if format_pattern.match(response):\n",
    "            score = 10\n",
    "        #soft format reward\n",
    "        elif format_pattern.search(response):\n",
    "            score = 2\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f\"Format reward for this response is: {score}\")\n",
    "    \n",
    "    #for evaluation and epoch logging\n",
    "\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions=['''[PLAN]\n",
    "unstack the red block from on top of the green block\n",
    "put down the red block\n",
    "[PLAN END]''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format reward for this response is: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_reward(completions=completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planwllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
