
model:
  name: google/gemma-3-12b-it
  device_map: auto

training:
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: .1
  lr_scheduler_type: linear
  optim: 'adamw_torch'
  adam_epsilon: 1e-8
#logging and saving
  output_dir: /srv/chawak/planning-with-llms/results/SFT
  evaluation_strategy: epoch
  logging_strategy: epoch
  save_strategy: epoch
  bf16: true
  fp16: false
  report_to: tensorboard


peft:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  task_type: "CAUSAL_LM"
  target_modules: ['q_proj','v_proj','k_proj','o_proj','gate_proj','up_proj','down_proj']
  #,'fc1','fc2'