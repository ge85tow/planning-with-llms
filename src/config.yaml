
model:
  name: google/gemma-2-9b-it
  device_map: auto

training:
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: .1
  lr_scheduler_type: linear
  optim: 'adamw_torch'
  adam_epsilon: 1e-8
#logging and saving
  output_dir: ../results/SFT/
  logging_steps: 50
  save_steps: 500
  eval_steps: 500
  evaluation_strategy: steps
  save_strategy: steps
  bf16: true
  fp16: false
  report_to: tensorboard


peft:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  task_type: "CAUSAL_LM"