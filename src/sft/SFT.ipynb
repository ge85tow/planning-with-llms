{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma3ForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/srv/chawak/planning-with-llms/src\")\n",
    "import shared.llm_utils as llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#name='google/gemma-3-12b-it'\n",
    "#cache_dir='/home/chawak/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8aafd13e10c467089901be1afe1137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Tokenize the dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split='train'\n",
    "n=3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  SFT: 1-1 Gemma-3 tokenizing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset_path=f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks'\n",
    "#dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "data=data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan'])\n",
    "print(f'Loaded HuggingFace dataset : {data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EOS token:\", tokenizer.eos_token)\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# train_text={'prompt':['I am cat'],\n",
    "#             'gold_plan':['I eat fish']}\n",
    "# val_text={'prompt':['I am dog'],\n",
    "#           'gold_plan':['I eat chicken']}\n",
    "# train_df=pd.DataFrame(data=train_text)\n",
    "# val_df=pd.DataFrame(data=val_text)\n",
    "# train_df.to_csv('../data/toy_train')\n",
    "# val_df.to_csv('../data/toy_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the TOY dataset\n",
    "# import numpy as np\n",
    "# split='val'\n",
    "# #dataset_path=f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks'\n",
    "# dataset_path=f'../data/toy_{split}'\n",
    "# data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "# data=data.remove_columns(['Unnamed: 0'])\n",
    "# print(f'Loaded HuggingFace dataset : {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function for tokenizing\n",
    "def tokenize_and_mask_function(examples):\n",
    "    #concatinate prompt and gold plan within our template\n",
    "    merged_inputs=[f\"{p[:-2]}{g[6:]}\" for p,g in zip(examples['prompt'],examples['gold_plan'])]\n",
    "\n",
    "    #tokenize concatinated input\n",
    "    tokenized=tokenizer(\n",
    "        merged_inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        padding_side='right',\n",
    "        max_length=1200\n",
    "    )\n",
    "\n",
    "    #tokenize ONLY the prompts and get their lengths\n",
    "    tokenized_prompts=tokenizer(\n",
    "        examples['prompt'],\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    #print(f'Tokenized prompts: {tokenized_prompts}')\n",
    "    prompt_lens = [len(ptoken) for ptoken in tokenized_prompts['input_ids']]\n",
    "    #print(\"Lengths of tokenized prompts\", prompt_lens)\n",
    "    \n",
    "    #estimating input token sequence lengths\n",
    "    merged_input_lens=[len(merged) for merged in tokenized['input_ids']]\n",
    "    sorted_lens=sorted(merged_input_lens, reverse=True)\n",
    "    #print('Highest lengths of merged-input token sequence:',sorted_lens[:5])\n",
    "    #print(\"Number of prompts tokenized\",len(prompt_lens))\n",
    "    \n",
    "    #masking prompt tokens for the labels \n",
    "    labels=[]\n",
    "    for input_ids, prompt_length in zip(tokenized['input_ids'],prompt_lens):\n",
    "        label=input_ids.copy()\n",
    "        #mask prompt tokens as -100 & adjustment for prompt template\n",
    "        label[:prompt_length-4]=[-100]*prompt_length\n",
    "        label=label[:-4]\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized['labels']=labels\n",
    "\n",
    "    tokenized['input_ids'] = [np.array(ids, dtype=np.int64) for ids in tokenized['input_ids']]\n",
    "    tokenized['labels']=[np.array(labels, dtype=np.int64) for labels in tokenized['labels']]\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map tokenizer function to our dataset\n",
    "tokenized_data=data.map(tokenize_and_mask_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "# data=load_dataset(\"csv\",data_files={split:dataset_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "# data=pd.read_csv(path)\n",
    "# data=data.drop(columns=['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=f'../data/{n}_blocks/SFT_contaminated_test_{n}_blocks_fullPlan'\n",
    "# cont=pd.read_csv(path)\n",
    "# cont=cont.drop(columns=['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "# cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.concat([data,cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_data=cont_data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan',])\n",
    "# cont_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ip=tokenized_data[split][0]['input_ids']\n",
    "encoded_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_ip=tokenizer.decode(encoded_ip)\n",
    "print(decoded_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_op=tokenized_data[split][0]['labels']\n",
    "encoded_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_op=tokenizer.decode([token for token in encoded_op if token != -100])\n",
    "print(decoded_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/toy_label_nopad\")\n",
    "tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/{n}_blocks/tokenized_dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  GRPO: 1-1 Gemma-3 tokenizing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "import numpy as np\n",
    "\n",
    "#dataset_path=f'/srv/chawak/planning-with-llms/data/{n}_blocks/GRPO_full_{split}_{n}_blocks'\n",
    "dataset_path=f'/srv/chawak/planning-with-llms/data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "#dataset_path=f'/srv/chawak/planning-with-llms/data/{n}_blocks/SFT_full_{split}_{n}_blocks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace dataset : DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['init', 'goal', 'prompt', 'gold_plan'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "data=data.remove_columns(['Unnamed: 0', 'demo_init', 'demo_goal', 'demo_plan'])\n",
    "print(f'Loaded HuggingFace dataset : {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token ID: 1\n",
      "Padding token ID: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"EOS token ID:\", tokenizer.eos_token_id)\n",
    "print(\"Padding token ID:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='''I am a blocksworld plan generator.\n",
    "I first think about the reasoning process in the mind and then provide the user with the plan.\n",
    "The reasoning process and plan are enclosed within <think> </think> and [PLAN] [PLAN END] tags, respectively,\n",
    "i.e., <think> reasoning process here </think> [PLAN] plan here [PLAN END].\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function for tokenizing\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    #apply chat template before tokenizing\n",
    "    prompts = [tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ) for prompt in examples[\"prompt\"]]\n",
    "\n",
    "    print(prompts[:1])\n",
    "\n",
    "    #tokenize chat-version of prompt\n",
    "    tokenized_prompts=tokenizer(\n",
    "        prompts\n",
    "    )\n",
    "\n",
    "    #print(f'Tokenized prompts: {tokenized_prompts}')\n",
    "    prompt_lens = [len(ptoken) for ptoken in tokenized_prompts['input_ids']]\n",
    "    print(\"Lengths of tokenized prompts\", prompt_lens)\n",
    "    \n",
    "    return tokenized_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags=\"Answer within the [PLAN] [PLAN END] tags.\"\n",
    "think=\"I first think about the reasoning process in the mind and then provide the user with the plan.\"\n",
    "think+=\" The reasoning process and plan are enclosed within <think> </think> and [PLAN] [PLAN END] tags, respectively,\"\n",
    "think+=\" i.e., <think> reasoning process here </think> [PLAN] plan here [PLAN END].\"\n",
    "\n",
    "def add_additional_string(sample):\n",
    "    \n",
    "    sample['prompt']=sample['prompt'][:-8]\n",
    "    print(f\"{sample['prompt']}\")\n",
    "    sample['prompt']+=think\n",
    "    print(f\"{sample['prompt']}\")\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.map(add_additional_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165702269a1e43f489a2c9174c16a6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user\\nI am a blocksworld plan generator.\\nI first think about the reasoning process in the mind and then provide the user with the plan.\\nThe reasoning process and plan are enclosed within <think> </think> and [PLAN] [PLAN END] tags, respectively,\\ni.e., <think> reasoning process here </think> [PLAN] plan here [PLAN END].\\n\\n\\nI am playing with a set of blocks where I need to arrange the blocks into stacks\\n        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\\n        I have the following restrictions on my actions:\\n        I can only pick up or unstack one block at a time\\n        I can only pick up or unstack a block if my hand is empty\\n        I can only pick up a block if the block is on the table and the block is clear\\n        A block is clear if the block has no other blocks on top of it and if the block is not picked up\\n        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\\n        I can only unstack a block from on top of another block if the block I am unstacking is clear\\n        Once I pick up or unstack a block, I am holding the block\\n        I can only put down a block that I am holding\\n        I can only stack a block on top of another block if I am holding the block being stacked\\n        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\\n        Once I put down or stack a block, my hand becomes empty\\n        Once you stack a block on top of a second block, the second block is no longer clear\\n\\n[STATEMENT]\\nAs initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\\nMy goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nunstack the brown block from on top of the violet block\\nput down the brown block\\npick up the violet block\\nstack the violet block on top of the brown block\\npick up the teal block\\nstack the teal block on top of the violet block\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\\nMy goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\\n\\nMy plan is as follows: \\n\\n[PLAN]<end_of_turn>\\n<start_of_turn>model\\n']\n",
      "Lengths of tokenized prompts [668, 654, 659, 665, 674, 659, 635, 681, 671, 648, 684, 648, 674, 674, 668, 674, 654, 659, 687, 684, 664, 668, 659, 656, 651, 638, 668, 687, 674, 661, 671, 668, 667, 674, 677, 651, 662, 652, 668, 671, 659, 659, 632, 671, 667, 668, 671, 657, 668, 674, 655, 658, 673, 638, 664, 687, 658, 655, 654, 674, 671, 668, 674, 668, 652, 638, 671, 656, 659, 651, 671, 670, 635, 651, 638, 674, 651, 651, 667, 684, 670, 662, 668, 659, 635, 635, 664, 674, 671, 662, 668, 652, 659, 690, 654, 638, 668, 665, 652]\n"
     ]
    }
   ],
   "source": [
    "#map tokenizer function to our dataset\n",
    "tokenized_data=data.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['init', 'goal', 'prompt', 'gold_plan', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8424aafa15e4de49ff2fb026074b869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/{n}_blocks/GRPO_systhink_tokenized_dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1-2 Dummy data tokenizing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=[2, 236777,   1006,   4799, 236777,   9039,  12480,      1,   -100, -100]\n",
    "ip=[     2, 236777,   1006,   4799, 236777,   9039,  12480,      1,      0,\n",
    "             0]\n",
    "decoded_op=tokenizer.decode([token for token in op if token != -100])\n",
    "print(f'Decoded labels : {decoded_op}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying encoded file storage\n",
    "from datasets import Dataset\n",
    "split='val'\n",
    "n=3\n",
    "# /srv/chawak/planning-with-llms/data/3_blocks/tokenized_dataset/test/\n",
    "# ds= Dataset.from_file(f'/srv/chawak/planning-with-llms/data/{n}_blocks/tokenized_dataset/{split}/data-00000-of-00001.arrow')\n",
    "ds=Dataset.from_file(f\"/srv/chawak/planning-with-llms/data/{n}_blocks/GRPO_tags_tokenized_dataset/{split}/data-00000-of-00001.arrow\")\n",
    "#ds= Dataset.from_file(f'/srv/chawak/planning-with-llms/data/toy_label_nopad/{split}/data-00000-of-00001.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input=ds[0]['input_ids']\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels=ds[0]['labels']\n",
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_ip=tokenizer.decode(encoded_input)\n",
    "decoded_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_op=tokenizer.decode([token for token in encoded_labels if token != -100])\n",
    "decoded_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define hyperparamters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import TrainingArguments, AdamW\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "n=3\n",
    "split='train'\n",
    "data_path=f'../data/{n}_blocks/tokenized_dataset/{split}'\n",
    "train_data=load_from_disk(data_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "n=3\n",
    "split='val'\n",
    "data_path=f'../data/{n}_blocks/tokenized_dataset/{split}'\n",
    "eval_data=load_from_disk(data_path)\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='../results/SFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(cfg['model']['name'],cache_dir=cache_dir)\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    cfg['model']['name'],\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=cfg['model']['device_map'],\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=Dataset.from_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get config file\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg=yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LORA config \n",
    "peft_args=LoraConfig(\n",
    "    r=cfg['peft']['r'],\n",
    "    lora_alpha=cfg['peft']['lora_alpha'],\n",
    "    lora_dropout=cfg['peft']['lora_dropout'],\n",
    "    task_type=cfg['peft']['task_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get LORA model\n",
    "lora_model=get_peft_model(model,peft_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_layers=lora_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=SFTConfig(\n",
    "    output_dir=cfg['training']['output_dir']+f'/{n}_blocks',\n",
    "    num_train_epochs=cfg['training']['num_train_epochs'],\n",
    "    per_device_train_batch_size= cfg['training']['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps=cfg['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=cfg['training']['learning_rate'],\n",
    "    weight_decay=cfg['training']['weight_decay'],\n",
    "    warmup_ratio=cfg['training']['warmup_ratio'],\n",
    "    #adam_epsilon=cfg['training']['adam_epsilon'],\n",
    "    #optim=cfg['training']['optim'],\n",
    "    logging_steps=cfg['training']['logging_steps'],\n",
    "    save_steps=cfg['training']['save_steps'],\n",
    "    eval_steps=cfg['training']['eval_steps'],\n",
    "    evaluation_strategy=cfg[\"training\"][\"evaluation_strategy\"],\n",
    "    save_strategy=cfg[\"training\"][\"save_strategy\"],\n",
    "    fp16=cfg[\"training\"][\"fp16\"],\n",
    "    bf16=cfg[\"training\"][\"bf16\"],\n",
    "    report_to=cfg[\"training\"][\"report_to\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer args dictionary\n",
    "optimizer = {\n",
    "    'params': lora_layers,\n",
    "    'lr': float(cfg['training']['learning_rate']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_args,\n",
    "    optimizer_cls_and_kwargs=(AdamW,optimizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Infer on the trained model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='/srv/chawak/planning-with-llms/results/SFT/training/training_21-05/checkpoint-8172'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=base_model #init model is base\n",
    "base_dir='/srv/chawak/planning-with-llms/results/SFT'\n",
    "model_path=base_dir+f'/training/training_21-05/checkpoint-8172'\n",
    "peft_model=PeftModel.from_pretrained(model,model_path,is_trainable=False,adapter_name=\"default\")\n",
    "print(f'\\n\\n++++++++ Loading model from: {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f'''A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "User:{question}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input, processor=llm_utils.get_tokenized_input(prompt=prompt,model=peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r= llm_utils.query_local_model(tokenized_input=tokenized_input,processor=processor,model=model,temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual mode\n",
    "with torch.no_grad():\n",
    "    outputs=model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0,\n",
    "    )\n",
    "print(f'Response from LLM: {tokenizer.decode(outputs[0][input_len:],skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question='''I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
    "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
    "        I have the following restrictions on my actions:\n",
    "        I can only pick up or unstack one block at a time\n",
    "        I can only pick up or unstack a block if my hand is empty\n",
    "        I can only pick up a block if the block is on the table and the block is clear\n",
    "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
    "        Once I pick up or unstack a block, I am holding the block\n",
    "        I can only put down a block that I am holding\n",
    "        I can only stack a block on top of another block if I am holding the block being stacked\n",
    "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
    "        Once I put down or stack a block, my hand becomes empty\n",
    "        Once you stack a block on top of a second block, the second block is no longer clear\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the green block is clear,  the hand is empty, the red block is on the table, the pink block is on top of the red block, the green block is on top of the pink block.\n",
    "My goal is to have that  the green block is on the table, the red block is on top of the green block, the pink block is on top of the red block.\n",
    "\n",
    "My plan is as follows:\n",
    "\n",
    "[PLAN]\n",
    "unstack the green block from on top of the pink block\n",
    "put down the green block\n",
    "unstack the pink block from on top of the red block\n",
    "put down the pink block\n",
    "pick up the red block\n",
    "stack the red block on top of the green block\n",
    "pick up the pink block\n",
    "stack the pink block on top of the red block\n",
    "[PLAN END]\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the brown block is clear,  the hand is empty, the violet block is on the table, the teal block is on top of the violet block, the brown block is on top of the teal block.\n",
    "My goal is to have that  the violet block is on the table, the teal block is on the table, the brown block is on the table.\n",
    "\n",
    "My plan is as follows: \n",
    "\n",
    "[PLAN]\n",
    "Answer within [PLAN] [PLAN END] tags.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de=tokenizer.decode([235309,\n",
    "  42446,\n",
    "  16960,\n",
    "  235307])\n",
    "print(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features,Sequence, Value\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "split='val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "import numpy as np\n",
    "\n",
    "#dataset_path=f'../data/{n}_blocks/SFT_full_{split}_{n}_blocks'\n",
    "dataset_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "data=load_dataset(\"csv\",data_files={split:dataset_path})\n",
    "data=data.remove_columns(['Unnamed: 0', 'init', 'goal', 'demo_init', 'demo_goal', 'demo_plan'])\n",
    "print(f'Loaded HuggingFace dataset : {data}')\n",
    "\n",
    "#helper function for tokenizing\n",
    "def tokenize_and_mask_function(examples):\n",
    "    #concatinate prompt and gold plan within our template\n",
    "    merged_inputs=[f\"{p[:-2]}{g[6:]}\" for p,g in zip(examples['prompt'],examples['gold_plan'])]\n",
    "\n",
    "    #tokenize concatinated input\n",
    "    tokenized=tokenizer(\n",
    "        merged_inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        padding_side='right',\n",
    "        max_length=1200\n",
    "    )\n",
    "\n",
    "    #tokenize ONLY the prompts and get their lengths\n",
    "    tokenized_prompts=tokenizer(\n",
    "        examples['prompt'],\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    #print(f'Tokenized prompts: {tokenized_prompts}')\n",
    "    prompt_lens = [len(ptoken) for ptoken in tokenized_prompts['input_ids']]\n",
    "    #print(\"Lengths of tokenized prompts\", prompt_lens)\n",
    "    \n",
    "    #estimating input token sequence lengths\n",
    "    merged_input_lens=[len(merged) for merged in tokenized['input_ids']]\n",
    "    sorted_lens=sorted(merged_input_lens, reverse=True)\n",
    "    #print('Highest lengths of merged-input token sequence:',sorted_lens[:5])\n",
    "    #print(\"Number of prompts tokenized\",len(prompt_lens))\n",
    "    \n",
    "    #masking prompt tokens for the labels \n",
    "    labels=[]\n",
    "    for input_ids, prompt_length in zip(tokenized['input_ids'],prompt_lens):\n",
    "        label=input_ids.copy()\n",
    "        #mask prompt tokens as -100 & adjustment for prompt template\n",
    "        label[:prompt_length-4]=[-100]*prompt_length\n",
    "        label=label[:-4]\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized['labels']=labels\n",
    "\n",
    "    tokenized['input_ids'] = [np.array(ids, dtype=np.int64) for ids in tokenized['input_ids']]\n",
    "    tokenized['labels']=[np.array(labels, dtype=np.int64) for labels in tokenized['labels']]\n",
    "    return tokenized\n",
    "\n",
    "#map tokenizer function to our dataset\n",
    "tokenized_data=data.map(tokenize_and_mask_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = Features({\n",
    "    \"prompt\": Value(\"string\"),\n",
    "    \"gold_plan\": Value(\"string\"),\n",
    "    \"input_ids\": Sequence(Value(\"int64\")),\n",
    "    \"attention_mask\": Sequence(Value(\"int64\")),\n",
    "    \"labels\": Sequence(Value(\"int64\")),\n",
    "})\n",
    "\n",
    "tokenized_data = tokenized_data.cast(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checks on tokenized data\n",
    "print(f'Data dictonary after tokenization: {tokenized_data}')\n",
    "#decoding for merged-text\n",
    "encoded_text=tokenized_data[split][0]['input_ids']\n",
    "print(f'Length of encoded merged-text : {len(encoded_text)}')\n",
    "decoded_text=tokenizer.decode(encoded_text, skip_special_tokens=False)\n",
    "print(f'Length of decoded merged-text : {len(encoded_text)}')\n",
    "print(f'Decoded merged-text: {decoded_text}')\n",
    "#deocding for lables\n",
    "encoded_text=tokenized_data[split][0]['labels']\n",
    "print(f'Length of encoded labels : {len(encoded_text)}')\n",
    "decoded_text=tokenizer.decode(\n",
    "    [token_id for token_id in encoded_text if token_id!=-100],\n",
    "    skip_special_tokens=True)\n",
    "print(f'Decoded labels: {decoded_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#save to file\n",
    "tokenized_data.save_to_disk(f\"/srv/chawak/planning-with-llms/data/{n}_blocks/tokenized_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_data['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "split='val'\n",
    "data_path=f'../data/{n}_blocks/tokenized_dataset/{split}'\n",
    "train_data=load_from_disk(data_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(train_data[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''I am playing with a set of blocks where I need to arrange the blocks into stacks\n",
    "        Here are the actions I can do: Pick up a block, Unstack a block from on top of another block, Put down a block, Stack a block on top of another block.\n",
    "        I have the following restrictions on my actions:\n",
    "        I can only pick up or unstack one block at a time\n",
    "        I can only pick up or unstack a block if my hand is empty\n",
    "        I can only pick up a block if the block is on the table and the block is clear\n",
    "        A block is clear if the block has no other blocks on top of it and if the block is not picked up\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block\n",
    "        I can only unstack a block from on top of another block if the block I am unstacking is clear\n",
    "        Once I pick up or unstack a block, I am holding the block\n",
    "        I can only put down a block that I am holding\n",
    "        I can only stack a block on top of another block if I am holding the block being stacked\n",
    "        I can only stack a block on top of another block if the block onto which I am stacking the block is clear\n",
    "        Once I put down or stack a block, my hand becomes empty\n",
    "        Once you stack a block on top of a second block, the second block is no longer clear\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the brown block is clear, the teal block is clear,  the hand is empty, the violet block is on the table, the brown block is on top of the violet block, the teal block is on the table.\n",
    "My goal is to have that  the brown block is on the table, the violet block is on top of the brown block, the teal block is on top of the violet block.\n",
    "\n",
    "My plan is as follows:\n",
    "\n",
    "[PLAN]\n",
    "unstack the brown block from on top of the violet block\n",
    "put down the brown block\n",
    "pick up the violet block\n",
    "stack the violet block on top of the brown block\n",
    "pick up the teal block\n",
    "stack the teal block on top of the violet block\n",
    "[PLAN END]\n",
    "\n",
    "[STATEMENT]\n",
    "As initial conditions I have that, the green block is clear,  the hand is empty, the pink block is on the table, the red block is on top of the pink block, the green block is on top of the red block.\n",
    "My goal is to have that  the pink block is on the table, the red block is on top of the pink block, the green block is on the table.\n",
    "\n",
    "My plan is as follows: \n",
    "[PLAN]\n",
    "\n",
    "Strategise at each step before you choose an action, then form a final plan. Answer within the [PLAN] [PLAN END] tags.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=train_data[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'input_ids': input_ids}\n",
    "response=model.generate(**inputs, max_new_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "for i in range(n):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load evaluation dataset\n",
    "import pandas as pd\n",
    "\n",
    "n=3\n",
    "split='train'\n",
    "data_path=f'../data/{n}_blocks/SFT_{split}_{n}_blocks_fullPlan'\n",
    "eval_data=pd.read_csv(data_path)\n",
    "eval_data=eval_data.drop(columns=['Unnamed: 0'])\n",
    "eval_data=eval_data.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer=llm_utils.get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags=\"Answer within the [PLAN] [PLAN END] tags.\"\n",
    "response=infer(model=peft_model,prompt=prompt,temp=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan='''[PLAN]\n",
    "1. Unstack brown from violet.\n",
    "2. Put down brown.\n",
    "3. Unstack violet from teal.\n",
    "4. Put down violet.\n",
    "5. Pick up teal.\n",
    "6. Stack teal on brown.\n",
    "[PLAN END]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed=llm_utils.parse_action_tuples(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_it in range(9,10):\n",
    "    print(model_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= llm_utils.load_tokenized_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(ds[0]['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "train=pandas.read_csv('/srv/chawak/planning-with-llms/data/4_blocks/SFT_train_4_blocks_fullPlan')\n",
    "contam_test=pandas.read_csv('/srv/chawak/planning-with-llms/data/4_blocks/SFT_contaminated_test_4_blocks_fullPlan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "contam_test.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contam_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=pandas.concat([train,contam_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('/srv/chawak/planning-with-llms/data/4_blocks/SFT_full_train_4_blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planwllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
